# uv run --with jupyter jupyter lab
# uv run jupyter nbconvert --to python study.ipynb
1 æ¨¡å‹
1.1 æ¨¡å‹è°ƒç”¨
import os
from dotenv import load_dotenv
from langchain_core.messages import content
# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()
os.environ['NO_PROXY'] = 'api.openai.rnd.huawei.com'
model_name = os.getenv("LLM_MODEL_ID")
api_key = os.getenv("LLM_API_KEY")
base_url = os.getenv("LLM_BASE_URL")
embeddings_model = os.getenv("EMBED_MODEL_NAME")
embeddings_api_key = os.getenv("EMBED_API_KEY")
# print(f"model_name: {model_name}")
# print(f"api_key: {api_key}")
# print(f"base_url: {base_url}")

from langchain.messages import HumanMessage,SystemMessage,AIMessage
from langchain.chat_models import init_chat_model

import httpx
# åˆ›å»ºåŒæ­¥å’Œå¼‚æ­¥å®¢æˆ·ç«¯ï¼Œéƒ½ç¦ç”¨SSLéªŒè¯
sync_client = httpx.Client(verify=False, timeout=20)
async_client = httpx.AsyncClient(verify=False, timeout=20)
model = init_chat_model(f"openai:{model_name}",  # æˆ–ä½¿ç”¨ "openai:local-model-name"
    base_url=base_url,  # æœ¬åœ°æœåŠ¡åœ°å€
    api_key=api_key, 
    http_client=sync_client,
    http_async_client=async_client)
# model = init_chat_model("openai:deepseek-chat", api_key="sk-7a2e144e3ab849a99e2ff237489a9225", http_client=client,
#     base_url="https://api.deepseek.com")

# é‡è¯•æœºåˆ¶ å½“ä½¿ç”¨ with_retry() æ–¹æ³•åï¼ŒåŸå§‹çš„ model ä¼šè¢«åŒ…è£…æˆä¸€ä¸ª RunnableRetry å¯¹è±¡
model = model.with_retry(
    wait_exponential_jitter = True, # å¼€å¯æŒ‡æ•°é€€é¿åŠéšæœºæŠ–åŠ¨
    stop_after_attempt = 3 # æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæŒ‡æ•°é€€é¿ç­‰å¾…æ—¶é—´1-ã€‹2-ã€‹4-ã€‹8
)
æœ¬åœ°åµŒå…¥æ–¹æ³•
1.ollamaå®‰è£…
2.æ¨¡å‹å®‰è£…æ–¹æ³•äºŒé€‰ä¸€
æ‰‹åŠ¨æ–¹å¼ï¼š
http://ollama.rnd.huawei.com/v2/manifests/registry.ollama.ai/library/ 
å†…ç½‘ä¸‹è½½æ¨¡å‹å¹¶æ”¾åˆ° 
macOS:Â ~/.ollama/models
Linux:Â /usr/share/ollama/.ollama/models
Windows:Â C:\Users\%username%\.ollama\models
è‡ªåŠ¨æ–¹å¼ï¼š
ollama pull ollama.rnd.huawei.com/library/bge-m3 --insecure 
3.æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹
(Invoke-WebRequest -method GET -uri http://localhost:11434/v1/models ).Content | ConvertFrom-json 
4.è°ƒç”¨æ¨¡å‹æµ‹è¯•
(Invoke-WebRequest -method POST -Body '{"model":"granite-embedding:latest", "input":"Why is the sky blue?"}' -uri http://localhost:11434/api/embed ).Content | ConvertFrom-json

import requests
import json
from typing import List, Optional, Dict, Any
from langchain_core.embeddings import Embeddings
from pydantic import ConfigDict
class SimpleOllamaEmbedder(Embeddings):
    """æœ€ç®€å•çš„ Ollama åµŒå…¥å™¨"""
    
    def __init__(self, model="granite-embedding", base_url="http://localhost:11434"):
        self.model = model
        self.base_url = base_url.rstrip('/')
        # åˆ›å»º session å¹¶ç¦ç”¨ SSL éªŒè¯
        self.session = requests.Session()
        self.session.verify = False
        self.session.trust_env = False
    
    def embed_query(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        url = f"{self.base_url}/api/embed"
        
        payload = {
            "model": self.model,
            "input": text 
        }
        
        try:
            print(f"   å‘é€è¯·æ±‚: {json.dumps(payload, ensure_ascii=False)[:100]}...")
            response = self.session.post(
                url, 
                json=payload, 
                timeout=30,
                # headers={'Content-Type': 'application/json'}
            )
            vector = response.json()["embeddings"]
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šå¤„ç† granite-embedding çš„ç‰¹æ®Šæ ¼å¼
            if isinstance(vector, list):
                # granite-embedding è¿”å›çš„æ˜¯ [[vector]]ï¼Œéœ€è¦è§£åŒ…
                if len(vector) > 0 and isinstance(vector[0], list):
                    vector = vector[0]  # å–å†…éƒ¨åˆ—è¡¨
                
                # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°åˆ—è¡¨
                if len(vector) > 0:
                    try:
                        vector = [float(x) for x in vector]
                    except:
                        vector = []
            
            return vector
        except Exception as e:
            print(f"âŒ åµŒå…¥å¤±è´¥: {e}")
            return []

    def embed_documents(
        self, texts: list[str], chunk_size: int | None = None, **kwargs: Any
    ) -> list[list[float]]:
        embeddings = []
    
        for text in texts:
            vector = self.embed_query(text)
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šå¤„ç† granite-embedding çš„ç‰¹æ®Šæ ¼å¼
            if isinstance(vector, list):
                # granite-embedding è¿”å›çš„æ˜¯ [[vector]]ï¼Œéœ€è¦è§£åŒ…
                if len(vector) > 0 and isinstance(vector[0], list):
                    vector = vector[0]  # å–å†…éƒ¨åˆ—è¡¨
                
                # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°åˆ—è¡¨
                if len(vector) > 0:
                    try:
                        vector = [float(x) for x in vector]
                    except:
                        vector = []
            
            embeddings.append(vector)
        
        return embeddings




    def get(self):
        url = f"{self.base_url}/v1/models"
        try:
            response = self.session.get(
                url, 
                timeout=30,
            )
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            raise
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"   çŠ¶æ€ç : {response.status_code}")
        print(f"   å“åº”å¤´: {dict(response.headers)}")
        print(f"   å“åº”å†…å®¹: {response.text}")
        print(f"   å“åº”å†…å®¹é•¿åº¦: {len(response.text)}")
        
        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code != 200:
            raise ValueError(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
        
        # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
        if not response.text or not response.text.strip():
            raise ValueError(f"å“åº”ä½“ä¸ºç©ºï¼ŒçŠ¶æ€ç : {response.status_code}")
        
        # å°è¯•è§£æ JSON
        try:
            return response.json()["data"]
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æå¤±è´¥: {e}")
            print(f"   çŠ¶æ€ç : {response.status_code}")
            print(f"   å“åº”ç±»å‹: {response.headers.get('Content-Type', 'unknown')}")
            print(f"   åŸå§‹å“åº”: {response.text}")
            raise ValueError(f"æ— æ³•è§£æ JSON å“åº”: {e}") from e


# ä½¿ç”¨ç¤ºä¾‹
def main():
    print("=" * 60)
    print("Ollama åµŒå…¥æµ‹è¯•")
    print("=" * 60)
    
    # ä½¿ç”¨ä½ æµ‹è¯•æˆåŠŸçš„æ¨¡å‹
    embedder = SimpleOllamaEmbedder(model="granite-embedding:latest")
    
    text = "æµ‹è¯•æ–‡æœ¬"
    # vector = embedder.get()
    # print(f"get {vector}")
    vector = embedder.embed_query(text)
    print(f"embed {vector}")


simple_embedder = SimpleOllamaEmbedder()

if __name__ == "__main__":
    main()
# ä¸åŒçš„æ¨¡å‹å¯èƒ½æ ¼å¼ä¸ä¸€æ ·å¯¼è‡´ä¸æ”¯æŒ
# from langchain_openai import OpenAIEmbeddings

# embed = OpenAIEmbeddings(
#     http_client = sync_client,
#     model="granite-embedding",
#     openai_api_base="http://localhost:11434/v1/",
#     openai_api_key="ollama",
# )

# res = embed.embed_query("Hello, world!")
# print(res)
# åµŒå…¥
from langchain.embeddings import init_embeddings
from langchain_community.embeddings import DashScopeEmbeddings

embeddings = DashScopeEmbeddings(
    model=os.getenv("EMBED_MODEL_NAME"), 
    dashscope_api_key=os.getenv("EMBED_API_KEY")
)

# embeddings = init_embeddings(
#     # model="openai:text-embedding-v4",
#     provider=os.getenv("openai"),
#     model=os.getenv("EMBED_MODEL_NAME"), 
#     api_key=os.getenv("EMBED_API_KEY")
# )

res = embeddings.embed_query("Hello, world!")
print(res[:10])
1.2 æç¤ºè¯
# æç¤ºè¯æ‹¼æ¥
from langchain_core.prompts import PromptTemplate,ChatPromptTemplate

prompt = PromptTemplate(
    template="{greeting}, {name}!",
    input_variables=["name"],
    partial_variables={"greeting": "Hello"}, #ç›¸å½“äºé»˜è®¤å‚æ•°
)

print(prompt.format(name="World"))

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}"),
])

messages = chat_prompt.format_messages(input="Hello, how are you?")
print(messages)

from langsmith import Client
import requests
session = requests.Session()
session.verify = False
prompt_client = Client(api_key=os.getenv("LANGSMITH_API_KEY"),session=session)
# ä»hubä¸Šè·å–æ¨¡æ¿ https://smith.langchain.com/hub æœ‰ç±»å‹åŒºåˆ†
prompt = prompt_client.pull_prompt("rlm/rag-prompt",include_model=True)
# print(prompt)
print(prompt.messages)

formated = prompt.format(
    context="b",
    question="a",
    )
print(formated)


1.3 æ‰¹å¤„ç†åŠè¾“å‡ºå¼•å¯¼
# æ™®é€šè°ƒç”¨æ‰¹å¤„ç†åŠç»“æ„åŒ–è¾“å‡ºå¼•å¯¼
# messages çš„æ‰§è¡Œé¡ºåºä¸ä¼˜å…ˆçº§(éå¸¸å…³é”®),
# LLM æŒ‰å¦‚ä¸‹é¡ºåºè§£æ:
# 1.system(æœ€é«˜ä¼˜å…ˆçº§),
# 2.developer(æ¨¡å‹çš„åŠŸèƒ½é€»è¾‘/å·¥ç¨‹çº¦æŸ)
# 3.user/human ç”¨æˆ·å½“å‰è¾“å…¥çš„ query
# 4.assistant å†å²å¯¹è¯
# 5.tool è°ƒç”¨
# æ¨¡å‹æ°¸è¿œä¼šå‚è€ƒå…¨éƒ¨ messages æ‰å¾—å‡ºæœ€ç»ˆè¾“å‡º
from langchain.messages import HumanMessage,SystemMessage,AIMessage
def test1():
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚"),
        HumanMessage(content="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Š")
    ]
    print("æ­£åœ¨è°ƒç”¨API...")
    response = model.invoke(messages)
    print("APIè°ƒç”¨å®Œæˆ")
    print(response.content)
    print(response.content_blocks) # å»ºè®®

    # # æµå¼è¾“å‡ºï¼Œä¸€æ¬¡æ˜¾ç¤ºä¸€ä¸ªå­—
    # for chunk in model.stream(messages):
    #     print(chunk.content, end="", flush=True)

    # # æµå¼è¾“å‡ºï¼Œæ¯æ¬¡æ˜¾ç¤ºå­˜é‡è¾“å‡º+å¢é‡è¾“å‡º
    # full = None
    # for chunk in model.stream(messages):
    #     full = chunk if full is None else full + chunk
    #     print(full.text)

    # # æŸ¥çœ‹å®Œæ•´å—å†…å®¹
    # print(full.content_blocks)

    # # æ‰¹å¤„ç†, æœ€å¤šå¹¶è¡Œ3ä¸ª
    # responses = model.batch(["question1", "question2", "question3"], concurrency=3)
    # # åœ¨æŸä¸ªä»»åŠ¡å®Œæˆä¹‹åç«‹é©¬è·å–ç»“æœ
    # for response in  model.batch_as_completed(["question1", "question2", "question3"], concurrency=3):
    #     print(response)

    # # ç»“æ„åŒ–ä¿¡æ¯å¼•å¯¼,...è¡¨ç¤ºå¿…å¡«å­—æ®µ,ä½¿ç”¨model_dump ç”Ÿæˆå­—å…¸æ ¼å¼æ•°æ®
    # from pydantic import BaseModel, Field
    # class Movie(BaseModel):
    #     title: str = Field(..., description="The title of the movie")
    #     year: int = Field(description="The year the movie was released", ge=2000,le=2002)#æœ€å°å€¼æœ€å¤§å€¼
    #     director: str = Field(description="The director of the movie")
    #     genre: str = Field(description="The genre of the movie")
    #     rating: float = Field(description="The rating of the movie")

    # model_with_structured_output = model.with_structured_output(Movie)
    # response = model_with_structured_output.invoke("What is the rating of the movie 'The Dark Knight'?")
    # print(type(response))
    # print(response)
    # print(response.model_dump())
test1()
1.5 æ ‡å‡†å¿«
# æ ‡å‡†å—  response.content_blocks ,æ”¯æŒå¤šæ¨¡æ€ç±»å‹ text, tool_call, image, audio, videoç­‰
# æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºé€šè¿‡æ•°æ®è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹çš„æŠ€æœ¯ã€‚
# [{'type': 'text', 'text': 'æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºé€šè¿‡æ•°æ®è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹çš„æŠ€æœ¯ã€‚'}]

# ä½œä¸ºå…¥å‚å‚è€ƒï¼š
messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚"),
        HumanMessage(
            content=[
                {"type": "text", "text": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Š"},
                {"type": "image_url", 
                "image_url": {"url": "https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png"},
                "mime_type": "image/png",
                "metadata": {"title": "ç™¾åº¦logo"}
                }
            ]
        )
    ]


for cb in messages[1].content_blocks:
    print(cb)

# æ ‡å‡†åˆ›å»ºæ ¼å¼å¦‚ä¸‹
# å†…å®¹å—åˆ›å»ºæ ‡å‡†æ ¼å¼
# å†…å®¹å—ç±»å‹	æ ‡å‡†æ ¼å¼(LangChain 1.0)
# æ–‡æœ¬  	  {"type": "text", "textâ€:""}
# å›¾åƒ	      {"type": "image", "url":"", "mime_type: "..."}
# éŸ³é¢‘	      {"type": "audio", "url":"", "mime_type: "..."}
# è§†é¢‘	      {"type": "vedio", "url":"", "mime_type: "..."}
# æ–‡ä»¶        {"type": "file", "url":"", "mime_type: "..."}
# Base64 å›¾åƒ {"type": "image", "base64":"", "mime_type: "..."}
# Base64 éŸ³é¢‘ {"type": "audio", "base64":"", "mime_type: "..."}
# OpenAI å›¾åƒ {"type": "image_url", "image_url":{"url":""}, "mime_type: "..."}

1.6 å¼‚æ­¥å¹¶å‘å¤„ç†RunableConfig
# å¼‚æ­¥å¹¶å‘å¤„ç†RunableConfig
async def main():
    from langchain_core.runnables import RunnableConfig
    # å¼‚æ­¥å¹¶å‘å¤„ç†RunableConfig
    config = RunnableConfig(
        max_concurrent=3,
        abstimeout=8, #å•ä¸ªä»»åŠ¡è¶…æ—¶æ—¶é—´
        metadata={"request_id": "1234567890","task":"query"} # è®°å½•ä»»åŠ¡è¯·æ±‚IDå’Œä»»åŠ¡ç±»å‹
        # callbacks # å¼‚å¸¸å›è°ƒ
    )
    responses = await model.abatch(["a","b","c"], config=config)

    for response in responses:
        print(response)

import asyncio
asyncio.run(main())
2 æ™ºèƒ½ä½“craete_agent
2.1 å·¥å…·è°ƒç”¨
![image.png](attachment:09af593d-4b11-4e58-96ae-fb37d180ded6.png)
# craete_agentåŠæµå¼è¾“å‡º
def test2():
    from langchain.agents import create_agent
    from langchain_tavily import TavilySearch 
    web_search = TavilySearch(max_results=3)
    tools = [web_search]
    agent = create_agent(tools=tools, model=model, 
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹.åœ¨å›ç­”è¿‡ç¨‹ä¸­å¦‚æœä½¿ç”¨äº†å·¥å…·,ä½ éœ€è¦è¯´æ˜ä¸€ä¸‹ä½¿ç”¨äº†å“ªäº›å·¥å…·.")
    messages = [
        # SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚"),
        HumanMessage(content="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿç›´æ¥ç»™æˆ‘å·¥å…·çš„ç»“æœå³å¯")
    ]
    
    print("æ­£åœ¨è°ƒç”¨API...")
    response = agent.invoke({"messages":messages})['messages']
    # print(response)
    for msg in response:
        print(type(msg), msg)
        print()
    #     if hasattr(msg, 'type') and hasattr(msg, 'content'):
    #         print(f'ğŸ”§ {msg.type}: {msg.content}')
    #     else:
    #         print(f'ğŸ”§ {type(msg).__name__}: {msg}')

    
    # # å…ˆæ‰“å°ç”¨æˆ·çš„æé—®
    # for msg in messages:
    #     if hasattr(msg, 'type') and hasattr(msg, 'content'):
    #         print(f'ğŸ”§ {msg.type}: {msg.content}')
    
    # # ä½¿ç”¨æµå¼è¾“å‡ºï¼Œç´¯ç§¯å®Œæ•´çš„å¥å­
    # current_content = ""
    # last_msg_type = None

    # stream_modeæ¨¡å¼
    # "values" (é»˜è®¤)æµå¼è¾“å‡ºæœ€ç»ˆç»“æœçš„å€¼ï¼Œå³messageåˆ—è¡¨
    # "messages" æµå¼è¾“å‡ºæ¶ˆæ¯å¯¹è±¡
    # "tools" æµå¼è¾“å‡ºå·¥å…·è°ƒç”¨è¿‡ç¨‹

    # for chunk in agent.stream({"messages": messages}, stream_mode="messages"):
    #     for msg in chunk:
    #         if hasattr(msg, 'type') and hasattr(msg, 'content'):
    #             msg_type = msg.type

    #             if not last_msg_type:
    #                 last_msg_type = msg_type
    #                 current_content = msg.content

    #             # å¦‚æœæ¶ˆæ¯ç±»å‹å˜åŒ–ï¼Œæ‰“å°ä¹‹å‰ç´¯ç§¯çš„å†…å®¹
    #             if last_msg_type != msg_type:
    #                 if current_content.strip():
    #                     print(f'ğŸ”§ {last_msg_type}: {current_content}')
    #                 current_content = ""
                
    #             # å¤„ç†échunkç±»å‹çš„æ¶ˆæ¯ï¼ˆå·¥å…·ï¼‰
    #             if not msg_type.endswith('Chunk'):
    #                 print(f'ğŸ”§ {msg_type}: {msg.content}')
    #                 current_content = ""
    #                 last_msg_type = None
    #             else:
    #                 # ç´¯ç§¯chunkå†…å®¹
    #                 current_content += msg.content
    #                 last_msg_type = msg_type
    #         else:
    #             # å¯¹äºæ²¡æœ‰typeå’Œcontentå±æ€§çš„æ¶ˆæ¯ï¼Œå¿½ç•¥
    #             if hasattr(msg, '__class__'):
    #                 # print(f'ğŸ”§ {type(msg).__name__}: {msg}')
    #                 pass
    
    # # æ‰“å°æœ€åç´¯ç§¯çš„å†…å®¹
    # if current_content.strip() and last_msg_type:
    #     print(f'ğŸ”§ {last_msg_type}: {current_content}')
# stream_modeæ¨¡å¼ï¼Œvalues
from langchain.agents import create_agent
from tools import *
tools = [get_info_by_sbom_from_pdm, how_to_get_info_by_spart, get_status_by_sbom_from_pdm]
agent = create_agent(tools=tools, model=model, 
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹,ä½ ä¼šä¼˜å…ˆå…³æ³¨å·¥ä½œæµç¨‹çš„å·¥å…·å³howå¼€å¤´çš„å·¥å…·,æ ¹æ®å¯¹åº”çš„è¿”å›å€¼å»è°ƒç”¨å¯¹åº”çš„å·¥å…·,å¦‚æœå·¥ä½œæµç¨‹å·¥å…·ä¸å­˜åœ¨,é‚£ä¹ˆä½ è‡ªå·±æ¨æµ‹åº”è¯¥ä½¿ç”¨ä»€ä¹ˆå·¥å…·.åœ¨å›ç­”è¿‡ç¨‹ä¸­å¦‚æœä½¿ç”¨äº†å·¥å…·,ä½ éœ€è¦è¯´æ˜ä¸€ä¸‹ä½¿ç”¨äº†å“ªäº›å·¥å…·.")
messages = [
    # SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚"),
    HumanMessage(content="03050STUçš„è¯¦ç»†ä¿¡æ¯æ˜¯ä»€ä¹ˆ")
]
config = {
    "configuration":{"thread_id":"1234567890"},
    "recursion_limit":10, # é™åˆ¶åº•å±‚ReActçš„é€’å½’æ·±åº¦
}
print("æ­£åœ¨è°ƒç”¨API...")
for event in agent.stream({"messages": messages}, stream_mode="values"):
    print(event["messages"][-1].content)
2.2 å¼‚æ­¥è¯­ä¹‰æ¨ç†åˆ†æ
# astream_events å¼‚æ­¥è¯­ä¹‰æ¨ç†åˆ†æï¼Œ è·å–LLMå†…éƒ¨çš„æ¨ç†æµç¨‹ï¼Œéœ€è¦æŒ‡å®šç‰ˆæœ¬ v1
async def main():
    from langchain.agents import create_agent
    agent = create_agent( model=model, 
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹.åœ¨å›ç­”è¿‡ç¨‹ä¸­å¦‚æœä½¿ç”¨äº†å·¥å…·,ä½ éœ€è¦è¯´æ˜ä¸€ä¸‹ä½¿ç”¨äº†å“ªäº›å·¥å…·.")
    messages = [
        # SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚"),
        HumanMessage(content="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
    ]
    # agent.astream_events() è¿”å›ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆasync generatorï¼‰ï¼Œ
    # éœ€è¦ä½¿ç”¨ async for æ¥è¿­ä»£è€Œä¸æ˜¯æ™®é€šçš„ for å¾ªç¯
    response = agent.astream_events({"messages":messages}, version="v1")
    async for event in response:
        print('-'*20)
        print(event['event'])
        for data in event['data']:
            print(event['data'])

# import asyncio
# asyncio.run(main())
 #jupyterå¯ä»¥ç›´æ¥è°ƒç”¨å¼‚æ­¥å‡½æ•°
await main()
2.3 å‚æ•°ä»‹ç»
create agent çš„æ ¸å¿ƒä»·å€¼åœ¨äºå®ƒé€šè¿‡â€œä¸‰è¦ç´ + ä¸‰æ‰©å±•â€çš„æç®€æŠ½è±¡,å½»åº•é‡æ„äº† Agent çš„å¼€å‘èŒƒå¼ã€‚æ‰€è°“ä¸‰è¦ç´ ,å³æ¨¡å‹(Model)ã€å·¥å…·(Tools)ä¸æç¤ºè¯
(System Prompt),è¿™ä¸‰è€…æ„æˆäº† Agent çš„"çµé­‚"â€”â€”å†³å®šäº†å®ƒèƒ½æ€è€ƒä»€ä¹ˆã€èƒ½åšä»€ä¹ˆä»¥åŠè¡Œä¸ºè¾¹ç•Œä½•åœ¨ã€‚è€Œä¸‰æ‰©å±•â€”â€”ä¸­é—´ä»¶(Middleware)ã€å†…å­˜ç®¡ç†
(Memory) ä¸çŠ¶æ€ç®¡ç† (State) -	åˆ™æ„å»ºäº† Agent çš„"ç¥ç»ç³»ç»Ÿ",ä½¿å…¶å…·å¤‡ç”Ÿäº§çº§åº”ç”¨æ‰€éœ€çš„å¯é æ€§ã€å¯è§‚æµ‹æ€§ä¸å¯ç»´æŠ¤æ€§ã€‚

å‚æ•°	        ç±»å‹        æ ¸å¿ƒä½œç”¨	 æœ€ä½³å®è·µ
model           str/å®ä¾‹    å¼•æ“        ç”Ÿäº§ç¯å¢ƒå®ä¾‹åŒ–é…ç½®
tools	        liste      æ‰§è¡Œèƒ½åŠ›	æè¿°æ¸…æ™°,æŒ‰éœ€æ·»åŠ 
system          prompt	   è¡Œä¸ºå‡†åˆ™	æ˜ç¡®è§’è‰²å’Œçº¦æŸ
middleware      liste	   åŠŸèƒ½æ‰©å±•	ç»„åˆæ—¥å¿—ã€å®‰å…¨ã€æ‘˜è¦
checkpointer	Saver	   çŸ­æœŸè®°å¿†	ç”Ÿäº§ç”¨ PostgresSaver
store	        Store	   é•¿æœŸè®°å¿†	è·¨ä¼šè¯ç”¨ PostgresStore
state           schema	   æ‰©å±•çŠ¶æ€	ç”¨ TypedDict é Pydantic
context         schema	   åŠ¨æ€ä¸Šä¸‹æ–‡ é…åˆ middleware ä½¿ç”¨
response_format	BaseModel  ç»“æ„åŒ–è¾“å‡º  API å¯¹æ¥åœºæ™¯å¯ç”¨
2.4 å‚æ•°ä¼ é€’config
# é€šè¿‡config ç»™create_agentåº•å±‚èŒƒå¼ä¼ é€’å‚æ•°
from langchain.agents import create_agent
from tools import *
tools = [get_info_by_sbom_from_pdm, how_to_get_info_by_spart, get_status_by_sbom_from_pdm]
agent = create_agent(tools=tools, model=model, 
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹,ä½ ä¼šä¼˜å…ˆå…³æ³¨å·¥ä½œæµç¨‹çš„å·¥å…·å³howå¼€å¤´çš„å·¥å…·,æ ¹æ®å¯¹åº”çš„è¿”å›å€¼å»è°ƒç”¨å¯¹åº”çš„å·¥å…·,å¦‚æœå·¥ä½œæµç¨‹å·¥å…·ä¸å­˜åœ¨,é‚£ä¹ˆä½ è‡ªå·±æ¨æµ‹åº”è¯¥ä½¿ç”¨ä»€ä¹ˆå·¥å…·.åœ¨å›ç­”è¿‡ç¨‹ä¸­å¦‚æœä½¿ç”¨äº†å·¥å…·,ä½ éœ€è¦è¯´æ˜ä¸€ä¸‹ä½¿ç”¨äº†å“ªäº›å·¥å…·.")
messages = [
    # SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚"),
    HumanMessage(content="03050STUçš„è¯¦ç»†ä¿¡æ¯æ˜¯ä»€ä¹ˆ")
]
config = {
    "configuration":{"thread_id":"1234567890"},
    "recursion_limit":10, # é™åˆ¶åº•å±‚ReActçš„é€’å½’æ·±åº¦
}
print("æ­£åœ¨è°ƒç”¨API...")
response = agent.invoke({"messages":messages},config=config)['messages']
for i  in response:
    print(i.type, i.content)
2.5 å·¥å…·æ³¨å†Œæ–¹å¼åŒºåˆ†
# @toolç®€å•å·¥å…·æ³¨å†Œæ–¹å¼
from tools import *
def test3():
    from langchain.agents import create_agent

    tools = [get_info_by_sbom_from_pdm, how_to_get_info_by_spart, get_status_by_sbom_from_pdm]
    agent = create_agent(tools=tools, model=model, 
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹,ä½ ä¼šä¼˜å…ˆå…³æ³¨å·¥ä½œæµç¨‹çš„å·¥å…·å³howå¼€å¤´çš„å·¥å…·,æ ¹æ®å¯¹åº”çš„è¿”å›å€¼å»è°ƒç”¨å¯¹åº”çš„å·¥å…·,å¦‚æœå·¥ä½œæµç¨‹å·¥å…·ä¸å­˜åœ¨,é‚£ä¹ˆä½ è‡ªå·±æ¨æµ‹åº”è¯¥ä½¿ç”¨ä»€ä¹ˆå·¥å…·.åœ¨å›ç­”è¿‡ç¨‹ä¸­å¦‚æœä½¿ç”¨äº†å·¥å…·,ä½ éœ€è¦è¯´æ˜ä¸€ä¸‹ä½¿ç”¨äº†å“ªäº›å·¥å…·.")
    messages = [
        # SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚"),
        HumanMessage(content="03050STUçš„è¯¦ç»†ä¿¡æ¯æ˜¯ä»€ä¹ˆ")
    ]
    print("æ­£åœ¨è°ƒç”¨API...")
    response = agent.invoke({"messages":messages})['messages']
    for i  in response:
        print(i.type, i.content)
# from langchain.tools import StructuredTool 
# è¿›è¡Œtoolåˆ›å»ºï¼Œæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥coroutineå‚æ•°
# æ”¯æŒå®Œæ•´çš„å‚æ•°æ ¡éªŒ
from langchain.tools import StructuredTool
from pydantic import BaseModel, Field
class DivInput(BaseModel):
    dividend: float = Field(..., description="è¢«é™¤æ•°")
    divisor: float = Field(..., description="é™¤æ•°")

def divide(input: DivInput) -> float:
    return input.dividend / input.divisor

divide_tool = StructuredTool.from_function(
    name="divide",
    description="ç”¨äºè®¡ç®—ä¸¤ä¸ªæ•°çš„å•†",
    func=divide,
    args_schema=DivInput,
    return_direct=True, # æ˜¯å¦ç›´æ¥è¿”å›å·¥å…·ç»“æœè¿˜æ˜¯LLMå†æ¬¡æ¢³ç†
    coroutine=False # æ˜¯å¦æ”¯æŒå¼‚æ­¥
)

# ä¹Ÿå¯ä»¥ç›´æ¥ç»§æ‰¿æ¥ç”Ÿæˆå·¥å…·
class DivTool(StructuredTool):
    name: str   = "divide"
    description: str = "ç”¨äºè®¡ç®—ä¸¤ä¸ªæ•°çš„å•†"
    args_schema: BaseModel = DivInput
    return_direct: bool = True
    coroutine: bool = False

    def _run(self, input: DivInput) -> float:
        return divide(input)
2.6 MCPæ–¹å¼æ¥å…¥å·¥å…·
# MCPå®¢æˆ·ç«¯çš„æ¥å…¥
# åˆ›å»ºè‡ªå®šä¹‰httpxå®¢æˆ·ç«¯å·¥å‚ï¼Œç¦ç”¨SSLéªŒè¯
def create_httpx_client_factory(headers=None, timeout=None, auth=None):
    """åˆ›å»ºç¦ç”¨SSLéªŒè¯çš„httpx.AsyncClientå·¥å‚å‡½æ•°"""
    return httpx.AsyncClient(
        verify=False,  # ç¦ç”¨SSLéªŒè¯
        timeout=timeout or httpx.Timeout(10.0),
        headers=headers,
        auth=auth
    )

from mcp.server.fastmcp import FastMCP
from langchain_mcp_adapters.client import MultiServerMCPClient

mcp_server_path = os.path.join("mcp_tools.py")
print(mcp_server_path)

mcp_client = MultiServerMCPClient({
    # æœ¬åœ°mcpæœåŠ¡å™¨-stdioä¼ è¾“
    "mcp_tool":{
        "transport": "stdio",
        "command": "python",
        "args":[mcp_server_path]
    },
    #å…¶ä»–æœåŠ¡å™¨
    # é«˜æ§åœ°å›¾ MCP æœåŠ¡
    # "amap-maps": {
    #     "thansport": "stdio",
    #     "command": "npx",
    #     "args":["-y", "@amap/amap-maps-mcp-server"],
    #     "env": {
    #         "AMAP_MAPS_API_KEY": os.getenv("AMAP_MAPS_API_KEY"),
    #     }
    # },
    # "amap-maps": {
    #   "transport": "sse",
    #   "url": "https://mcp.api-inference.modelscope.net/a0704a28438c44/sse",
    #   "timeout": 10,  # HTTPè¶…æ—¶10ç§’
    #   "sse_read_timeout": 30,  # SSEè¯»å–è¶…æ—¶30ç§’
    #   "httpx_client_factory": create_httpx_client_factory  # ä½¿ç”¨è‡ªå®šä¹‰å®¢æˆ·ç«¯å·¥å‚ç¦ç”¨SSLéªŒè¯
    # }
})

async def main():
    try:
        print("æ­£åœ¨è·å–MCPå·¥å…·...")
        mcp_tools = await mcp_client.get_tools()
        print(f"æˆåŠŸè·å– {len(mcp_tools)} ä¸ªå·¥å…·:")
        for tool in mcp_tools:
            print(f"  - {tool.name}: {tool.description}")
    except Exception as e:
        print(f"âŒ è·å–MCPå·¥å…·æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

# import asyncio
# asyncio.run(main())
await main()

2.7 æ„å›¾è¯†åˆ«
å·¥å…·è°ƒç”¨æ··ä¹±è§£å†³æ–¹æ¡ˆï¼šæ„å›¾è¯†åˆ«
ï¼ˆ1ï¼‰å¼•å…¥å·¥å…·è·¯ç”±ï¼Œæ„å›¾åˆ†ç±»æ¨¡å‹ ç”±ä¸€ä¸ªagentè´Ÿè´£å¯¹æ„å›¾è¿›è¡Œåˆ†ç±»
ï¼ˆ2ï¼‰å­—å…¸åŠ¨æ€åŠ è½½å·¥å…·ç»„ï¼Œæ¯ä¸ªå·¥å…·ç»„ç”Ÿæˆä¸€ä¸ªå­agentè¿›è¡Œå¤„ç†
ï¼ˆ3ï¼‰å»ºè®®ï¼šå·¥å…·åç§°åŠ¨è¯å¼€å¤´ï¼Œæ ‡å‡†åŒ–å…¥å‚ï¼ˆsql:strï¼‰ï¼Œæè¿°åŒ…æ‹¬ èƒ½å¹²å•¥ä¸èƒ½å¹²å•¥ï¼Œå…¸å‹è¾“å…¥ç¤ºä¾‹
ï¼ˆ4ï¼‰é‡‡ç”¨"å·¥å…·è¿‡æ»¤Prompt"ä¿®é¥°æ¨¡å‹è¡Œä¸ºï¼šä½ å¿…é¡»ä¸¥æ ¼æ ¹æ®å·¥å…·æè¿°é€‰æ‹©å·¥å…·ã€‚ ä¸èƒ½çŒœæµ‹å·¥å…·åŠŸèƒ½ã€‚ å¦‚æœæ²¡æœ‰åˆé€‚çš„å·¥å…·,è¯·å›ç­”"æ— åˆé€‚å·¥å…·"
2.8 ç³»ç»Ÿæç¤ºè¯
é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯,æ‚¨å¯ä»¥:
å®šä¹‰è§’è‰²:ä»å®¢æœåˆ°ä¸“å®¶,ä»æ•™å¸ˆåˆ°é¡¾é—®çš„
çº¦æŸè¾“å‡º:æ§åˆ¶é•¿åº¦ã€æ ¼å¼ã€è¯­è¨€
å¼•å¯¼å·¥å…·:å¼ºåˆ¶æˆ–å¯é€‰ä½¿ç”¨å·¥å…·
ä¿éšœå®‰å…¨:é˜²æ­¢æ•°æ®æ³„éœ²å’Œè¿è§„æ“ä½œ
å®ç°ä¸ªæ€§åŒ–:é€šè¿‡åŠ¨æ€æç¤ºæ”¯æŒå¤šç§Ÿæˆ·çš„
è®°ä½:åœ¨ LangChain 1.0 ä¸­,system_promptçš„è®¾è®¡è´¨æ˜¯ç›´æ¥å†³å®šäº† Agent çš„è¡¨ç°ä¸Šé™ã€‚æŠ•å…¥æ—¶é—´æ‰“ç£¨æç¤ºè¯,è¿œæ¯”è°ƒæ•´æ¨¡å‹å‚æ•°æ›´æœ‰æ•ˆã€‚

2.9 è®°å¿†ç®¡ç†
ä½ é›¾è¦æŒæ¡ä¸‰ä¸ªæ ¸å¿ƒè¦ç´ :
State (çŠ¶æ€):å®šä¹‰ç”¨æ¥å­˜å‚¨æ¶ˆæ¯çš„ç»“æ„(é€šå¸¸æ˜¯ MessagesState)
Checkpointer (æ£€æŸ¥ç‚¹ä¿å­˜å™¨):è´Ÿè´£åœ¨æ¯ä¸€æ­¥ç»“æŸåæŠŠçŠ¶æ€ä¿å­˜ä¸‹æ¥(çŸ­æœŸè®°å¿†é€šå¸¸ç”¨ MemorySaver)
Thread ID(çº¿ç¨‹ID):åœ¨è°ƒç”¨æ—¶é€šè¿‡ config ä¼ å…¥,ç”¨æ¥éš”ç¦»ä¸åŒç”¨æˆ·çš„å¯¹è¯ä¸Šä¸‹æ–‡ã€‚
2.9.1 çŸ­æœŸè®°å¿†
çŸ­æœŸè®°å¿†é€šè¿‡LangGraphçš„ AgentState (ä¸€ä¸ªTypedDict)æ¥ç®¡ç†ã€‚å¯¹è¯å†å²ã€ä¸­é—´æ­¥éª¤ç­‰ä¿¡æ¯è¢«ä¿å­˜åœ¨çŠ¶æ€ä¸­,å¹¶é€šè¿‡æ£€æŸ¥ç‚¹(Checkpoints)æœºåˆ¶åœ¨æ¯æ¬¡è¿­ä»£åæŒä¹…åŒ–ã€‚è¿™ä½¿å¾—é•¿å¯¹è¯å’Œå¤±è´¥æ¢å¤æˆä¸ºå¯èƒ½ã€‚
ï¼ˆ1ï¼‰Checkpointeræœºåˆ¶
è¿™æ˜¯ LangGraph è®°å¿†çš„çµé­‚ã€‚
Â· ä¸åŠ è¿™ä¸€è¡Œ:Agent æ˜¯æ— çŠ¶æ€çš„ã€‚æ¯æ¬¡ invokeéƒ½æ˜¯å…¨æ–°çš„å¼€å§‹ã€‚çš„
Â· åŠ ä¸Šè¿™ä¸€è¡Œ:LangGraph ä¼šåœ¨æ¯ä¸€æ­¥æ‰§è¡Œå,æŠŠ state åºåˆ—åŒ–å¹¶å­˜å…¥ MemorySaverã€‚
Â· åŸç†:å½“ä½ å†æ¬¡ invoke å¹¶ä¼ å…¥ thread id æ—¶,LangGraph ä¼šå…ˆå»å†…å­˜é‡ŒæŸ¥"è¿™ä¸ª ID ä¸Šæ¬¡åœåœ¨å“ªé‡Œ?çŠ¶æ€æ˜¯ä»€ä¹ˆ?",ç„¶ååŠ è½½çŠ¶æ€,æŠŠä½ çš„æ–°æ¶ˆæ¯,	append è¿›å»,å†ç»§ç»­è¿è¡Œã€‚
ï¼ˆ2ï¼‰Thread IDé…ç½®
è¿™æ˜¯çŸ­æœŸè®°å¿†çš„"é’¥åŒ™"ã€‚
Â· åœ¨ Web å¼€å‘ä¸­,è¿™å°±æ˜¯ Session IDã€‚
Â· ä½ éœ‡è¦ä¸ºæ¯ä¸ªç”¨æˆ·æˆ–æ¯ä¸ªä¼šè¯ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„IDã€‚
Â· ä¸åŒçš„ ID ä¹‹é—´å†…å­˜æ˜¯å®Œå…¨éš”ç¦»çš„(å¦‚ä»£ç ä¸­ session_user_123 å’Œ session_user 999 çš„åŒºåˆ«)ã€‚

InMemorySaver ç»“åˆ config
def test5():
    
    from langgraph.checkpoint.memory import InMemorySaver
    from langchain.agents import create_agent
    agent = create_agent(
        model=model,
        tools=[],
        checkpointer=InMemorySaver()
    )
    config = {"configurable":{"thread_id":"1234567890"}} 
    config1 = {"configurable":{"thread_id":"1234567891"}} 
    
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent
agent = create_agent(
    model=model,
    tools=[],
    checkpointer=InMemorySaver()
)
config = {"configurable":{"thread_id":"1234567890"}} 
config1 = {"configurable":{"thread_id":"1234567891"}} 

    # ç¬¬ä¸€æ¬¡è°ƒç”¨ - ä½¿ç”¨å­—å…¸æ ¼å¼ä¼ å…¥æ¶ˆæ¯
    print("=" * 60)
    print("ç¬¬ä¸€æ¬¡è°ƒç”¨ (thread_id: 1234567890)")
    print("=" * 60)
    response1 = agent.invoke(
        {"messages": [HumanMessage(content="æˆ‘å±…ä½åœ¨æ¾³å¤§åˆ©äºšæ‚‰å°¼")]}, 
        config=config
    )
    print("ç”¨æˆ·:", "æˆ‘å±…ä½åœ¨æ¾³å¤§åˆ©äºšæ‚‰å°¼")
    print("AIå“åº”:", response1["messages"][-1].content)
    print(f"æ¶ˆæ¯å†å²æ•°é‡: {len(response1['messages'])}")
    print()
    
    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šä½¿ç”¨ç›¸åŒçš„thread_idï¼Œåº”è¯¥èƒ½è®°ä½ç¬¬ä¸€æ¬¡çš„å¯¹è¯
    print("=" * 60)
    print("ç¬¬äºŒæ¬¡è°ƒç”¨ (thread_id: 1234567890 - åº”è¯¥è®°ä½ç¬¬ä¸€æ¬¡å¯¹è¯)")
    print("=" * 60)
    response2 = agent.invoke(
        {"messages": [HumanMessage(content="æˆ‘ç°åœ¨åœ¨å“ªé‡Œï¼Ÿ")]}, 
        config=config
    )
    print("ç”¨æˆ·:", "æˆ‘ç°åœ¨åœ¨å“ªé‡Œï¼Ÿ")
    print("AIå“åº”:", response2["messages"][-1].content)
    print(f"æ¶ˆæ¯å†å²æ•°é‡: {len(response2['messages'])}")
    print()
    
    # ç¬¬ä¸‰æ¬¡è°ƒç”¨ï¼šä½¿ç”¨ä¸åŒçš„thread_idï¼Œæ‰€ä»¥ä¸ä¼šè®°ä½ç¬¬ä¸€æ¬¡çš„å¯¹è¯
    print("=" * 60)
    print("ç¬¬ä¸‰æ¬¡è°ƒç”¨ (thread_id: 1234567891 - æ–°çš„å¯¹è¯ï¼Œä¸è®°å¾—ä¹‹å‰)")
    print("=" * 60)
    response3 = agent.invoke(
        {"messages": [HumanMessage(content="æˆ‘ç°åœ¨åœ¨å“ªé‡Œï¼Ÿ")]}, 
        config=config1
    )
    print("ç”¨æˆ·:", "æˆ‘ç°åœ¨åœ¨å“ªé‡Œï¼Ÿ")
    print("AIå“åº”:", response3["messages"][-1].content)
    print(f"æ¶ˆæ¯å†å²æ•°é‡: {len(response3['messages'])}")
    print()


ï¼ˆ3ï¼‰PostgresSaver()	æ•°æ®åº“æŒä¹…åŒ–è®°å¿†
PostgresSaver å³ä½¿å­˜å‚¨åˆ°æ•°æ®åº“,ä»ç„¶å±äºçŸ­æœŸè®°å¿†ã€‚
ä»ç„¶å°¾äºçŸ­æœŸè®°å¿†çš„åŸå› :
ä½œç”¨åŸŸé™åˆ¶:å®ƒåªæ£€ç´¢å’ŒåŠ è½½å½“å‰ thread id çš„æ•°æ®
Â·ç”Ÿå‘½å‘¨æœŸç®¡ç†:é»˜è®¤ä¸ä¼šä¸»åŠ¨æ¸…ç†,ä½†æ•°æ®è¯­ä¹‰ä¸Šå±‹äº"æœ¬æ¬¡ä¼šè¯"
Â·æ— è·¨ä¼šè¯æ£€ç´¢èƒ½åŠ›:æ— æ³•åœ¨æ–°ä¼šè¯ä¸­è‡ªåŠ¨è®¿é—®æ—§ä¼šè¯æ•°æ®(é™¤éæ‰‹åŠ¨æŒ‡å®šæ—§ thread id)

æ•°æ®åº“ï¼šhttps://www.postgresql.org/
uv add langgraph-checkpoint-postgres
![image.png](attachment:2f3c2d1e-ee6e-4d69-a5fb-afad35bf03c7.png)
# æµ‹è¯•æœ¬åœ°postgres è¿æ¥ https://get.enterprisedb.com/postgresql/postgresql-18.1-1-windows-x64.exe
import os
os.environ['PGPASSWORD'] = 'zkjiao'
!psql -U postgres -d postgres -c "SELECT version();"
from langchain.agents import create_agent
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_core.tools import tool

@tool
def get_user_info(user_name: str):
    "ç»™å®šç”¨æˆ·åï¼Œè¿”å›ç”¨æˆ·ä¿¡æ¯"
    db = {
        "å¼ ä¸‰":18,
    }
    return f"å§“åï¼š{user_name}ï¼Œå¹´é¾„ï¼š{db.get(user_name,0)}"

DB_URL = "postgresql://postgres:zkjiao@localhost:5432/postgres"

with PostgresSaver.from_conn_string(DB_URL) as checkpointer:
    # è‡ªåŠ¨åˆ›å»ºè¡¨ç»“æ„ï¼Œé¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨åˆ›å»º
    checkpointer.setup()

    agent = create_agent(
        checkpointer=checkpointer,
        tools=[get_user_info],
        model=model
    )

    config = {
        "configurable":{"thread_id":"user_lisi"}
    }
    response = agent.invoke({"messages": [HumanMessage(content="å¸®å¿™æŸ¥è¯¢ä¸€ä¸‹æå››çš„ä¿¡æ¯")]}, config=config)
    # print(agent.get_state(config)) # 
    print(response["messages"][-1].content)

# psql -U postgres -d postgres -c "SELECT * FROM checkpoints WHERE thread_id ='user_lisi' LIMIT 3;"
2.9.2 ä¸Šä¸‹æ–‡è£å‰ª trim_messages()

# ä¿®å¤ tiktoken ä¸‹è½½ç¼–ç æ–‡ä»¶æ—¶çš„ SSL è¯ä¹¦éªŒè¯é—®é¢˜
import requests
import urllib3
from functools import wraps

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ä¿å­˜åŸå§‹çš„ requests.get æ–¹æ³•
_original_get = requests.get

# åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œè‡ªåŠ¨ç¦ç”¨ SSL éªŒè¯
@wraps(_original_get)
def patched_get(*args, **kwargs):
    # å¦‚æœ verify å‚æ•°æœªè®¾ç½®ï¼Œåˆ™è®¾ç½®ä¸º False
    if 'verify' not in kwargs:
        kwargs['verify'] = False
    return _original_get(*args, **kwargs)

# æ›¿æ¢ requests.get
requests.get = patched_get

import tiktoken
# ä¸åŒçš„æ¨¡å¼ä½¿ç”¨ä¸åŒçš„ç¼–ç 

def get_token_encoder(model_name=None):
    return tiktoken.encoding_for_model(model_name) if model_name else tiktoken.get_encoding("o200k_base")


def count_tokens_tiktoken(messages):
    token_encoder = get_token_encoder()
    total_token = 0
    for message in messages:
        role_tokens = len(token_encoder.encode(message.type))
        content_tokens = len(token_encoder.encode(message.content))
        format_overhead = 4 
        total_token += role_tokens + content_tokens + format_overhead
    return total_token

# æ‰‹åŠ¨è£å‰ªç¤ºä¾‹
def invoke_with_trim(agent,user_input,config):
    state = agent.get_state(config)

    existing_messages = state.values.get("messages",[]) if state else []

    current_tokens = count_tokens_tiktoken(existing_messages)

    trimmed_messages = []
    if existing_messages:
        trimmed_messages = trim_messages(
            existing_messages, 
            max_tokens=300, #æœ€å¤§å…è®¸tokenæ•°
            token_counter=count_tokens_tiktoken, # è®¡ç®—tokenæ–¹æ³•
            strategy = "last", # è£å‰ªç­–ç•¥ï¼Œlastè¡¨ç¤ºä¿ç•™æœ€æ–°æ¶ˆæ¯ï¼Œfirstè¡¨ç¤ºä¿ç•™æœ€æ—©æ¶ˆæ¯
            include_system = True, # æ˜¯å¦ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            allow_partial = False, # falseè¡¨ç¤ºä¸å…è®¸éƒ¨åˆ†æ¶ˆæ¯ä¼šä¿ç•™æ¶ˆæ¯å®Œæ•´æ€§
            start_on = "human" # ä»humanæ¶ˆæ¯å¼€å§‹è£å‰ª
        )
    
        new_tokens = count_tokens_tiktoken(trimmed_messages)
        print(f"å½“å‰tokenæ•°ï¼š{current_tokens}, è£å‰ªåtokenæ•°ï¼š{new_tokens}")
    new_messages = [*trimmed_messages, user_input]
    print("è£å‰ªåçš„æ¶ˆæ¯ï¼š", new_messages[0])
    return agent.invoke({"messages":new_messages}, config)

config = {
    "configurable":{"thread_id":"user_lisi"}
}    
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent
agent = create_agent(
    model=model,
    tools=[],
    checkpointer=InMemorySaver()
)


user_input = [
    "æˆ‘æ˜¯å°æ˜ï¼Œä½ æ˜¯å°çº¢",
    "æˆ‘æ˜¯è°",
    "ä»–æ˜¯å°å…°ï¼Œå¥¹æ˜¯å°å•",
    "ä»–æ˜¯è°",
    "ä½ æ˜¯è°",
]

for i, query in enumerate(user_input):
    print(f"ç¬¬{i+1}è½®å¯¹è¯ï¼š{query}")
    response = invoke_with_trim(agent,query,config)
    print(response["messages"][-1].content)

2.9.3 è‡ªå®šä¹‰stateæ‰©å±•

import typing
from typing_extensions import TypedDict
from langchain_core.messages import BaseMessage
class ExtendedState(TypedDict):
    messages: list[BaseMessage]
    user_id: str	#æ‰©å±•:	ç”¨æˆ·èº«ä»½,ç”¨å¼€æƒè´¨æ§åˆ«å’Œä¸ªç»ä½‘
    session_id: str # æ‰©å±•ï¼šä¼šè¯æ ‡è¯†ï¼Œç”¨äºå¯¹è¯å†å²ç®¡ç†
    retry_count: int # æ‰©å±•:é‡è¯•æ¬¡æ•°ï¼Œç”¨äºå¼€å‘,ç”¨äºçººæµ‹å¤„ç†è´¨å¡”
    original_query: str	# æ‰©å±•:åŸå§‹æŸ¥è¯¢ï¼Œç”¨æˆ·æ—¥å¿—å’Œå®¡è®¡


ä½¿ç”¨TypedDict å½“ä¸”ä»…å½“ï¼š
æ€§èƒ½æ•æ„Ÿï¼Œé¿å…pydanticåºåˆ—åŒ–å¼€é”€
æ•°æ®ç®€å•ç®€å•æ— åµŒå¥—
ä»…éœ€ç±»å‹æç¤º
å¤–éƒ¨åº“è¦æ±‚

from langchain.agents import create_agent,AgentState
class CustomAgentState(AgentState):
    user_id: str
    preferences: dict
    visit_count:int

from langchain.tools import ToolRuntime, tool
from langgraph.types import Command
from langchain.messages import ToolMessage

@tool
def update_user_theme(run:ToolRuntime, theme:str) -> Command:
    '''æ›´æ–°ç”¨æˆ·ä¸»é¢˜'''
    # ToolRuntime æä¾›å¯¹stateå’Œcontentçš„è®¿é—®èƒ½åŠ›
    # ToolRuntime.state  å½“å‰çŠ¶æ€
    # ToolRuntime.context è°ƒç”¨ä¸Šä¸‹æ–‡
    # ToolRuntime.tool_call_id å·¥å…·è°ƒç”¨id
    current_theme = run.state.get("preferences", {})
    current_theme["theme"] = theme

    return Command(
        update={
            "preferences": current_theme,
            "messages":[
                ToolMessage(
                    content=f"æ›´æ–°ä¸»é¢˜ä¸º {theme}",
                    tool_call_id=run.tool_call_id
                )
            ]
        }
    )

@tool
def greet_user(run:ToolRuntime) -> Command:
    '''æ¬¢è¿ç”¨æˆ·'''
    user_id = run.state.get("user_id", "")
    theme = run.state.get("preferences", {})["theme"]
    return f"æ¬¢è¿å›æ¥{user_id},å½“å‰ä¸»é¢˜{theme}"

from langgraph.checkpoint.memory import InMemorySaver
agent = create_agent(
    model = model,
    state_schema=CustomAgentState,
    tools=[update_user_theme,greet_user],
    checkpointer=InMemorySaver()
)

config = {
    "configurable":{"thread_id":"1234567890"}
}
response = agent.invoke(
    {
        "messages":[HumanMessage(content="æ›´æ–°ä¸»é¢˜ä¸ºè“è‰²")],
        "user_id":"lisi",
        "preferences":{"language":"ä¸­æ–‡"}
    },
    config=config
)
print(response["messages"][-1].content)
response = agent.invoke(
    {
        "messages":[HumanMessage(content="æ¬¢è¿æˆ‘")]
    }, config=config
)
print(response["messages"][-1].content)

2.9.4 é•¿æœŸè®°å¿†
Milvus:å¼€æºå‘æ˜¯æ•°æ®åº“,æ”¯æŒå¤§è§„æ¨¡å‘é‡æ£€ç´¢
Qdrant:é«˜æ€§èƒ½å‘æ˜¯æœç´¢å¼•æ“
Pinecone:äº‘åŸç”Ÿå‘é‡æ•°æ®åº“æœåŠ¡çš„
Chroma:è½»æ˜¯çº§å‘æ˜¯æ•°æ®åº“,å¯æœ¬åœ°æŒä¹…åŒ–  uv add langchain-chroma

from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.tools import tool
from datetime import datetime

from dashscope_embeddings import DashScopeEmbeddings
import dashscope
import requests
# åˆ›å»ºè‡ªå®šä¹‰sessionå…³é—­SSLéªŒè¯
session = requests.Session()
session.verify = False

# è®¾ç½®dashscopeä½¿ç”¨è‡ªå®šä¹‰session
dashscope.default_session = session

embeddings = DashScopeEmbeddings(
    model=embeddings_model,
    api_key=embeddings_api_key, 
    http_client=sync_client  # ä½¿ç”¨ç›¸åŒçš„ http_clientï¼Œç¦ç”¨ SSL éªŒè¯å¹¶è®¾ç½®è¶…æ—¶
)

vector_store = Chroma(
    collection_name="test",
    embedding_function=embeddings,
    #persist_directory="chroma_db" #å¦‚æœæƒ³è¦å­˜å‚¨åˆ°ç¡¬ä»¶éœ€è¦å–æ¶ˆæ³¨é‡Šè¿™ä¸€è¡Œ
)

@tool 
def save_memory(content:str):
    '''ä¿å­˜è®°å¿†'''
    doc = Document(page_content=content, metadata={"source":"user","timestamp":datetime.now().isoformat()})
    vector_store.add_documents([doc])
    return "è®°å¿†ä¿å­˜æˆåŠŸ"

@tool
def search_memory(query:str):
    '''æ£€ç´¢è®°å¿†'''
    docs = vector_store.similarity_search(query, k=2)
    return docs

from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model=model,
    tools=[save_memory,search_memory],
    checkpointer=InMemorySaver(),
    system_prompt="ä½ æ˜¯ä¸€ä¸ªè®°å¿†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚å¦‚æœä½¿ç”¨äº†å·¥å…·ï¼Œè¯·çŸ¥ä¼šï¼Œå¹¶é«˜é€Ÿå·¥å…·ä½¿ç”¨ç»“æœ"
)

config = {"configurable":{"thread_id":"test1"}}
user_input = "æˆ‘å–œæ¬¢åƒè¥¿ç“œï¼Œå¯¹é¦™è•‰è¿‡æ•"
for chunk in agent.stream({"messages":[HumanMessage(content=user_input)]},config=config,stream_mode="values"):
    print("[chunk]",chunk["messages"][-1].content)

# è™½ç„¶configçº¿ç¨‹å·²ç»å˜åŒ–ï¼Œä½†æ˜¯ç”±äºé•¿æœŸå­˜å‚¨å‘é‡æ•°æ®åº“ï¼Œå®é™…ä¸Šä»ç„¶èƒ½è·å¾—ä¿¡æ¯
config = {"configurable":{"thread_id":"test2"}}
user_input = "æˆ‘å¯ä»¥åƒé¦™è•‰å—"
for chunk in agent.stream({"messages":[HumanMessage(content=user_input)]},config=config,stream_mode="values"):
    print("[chunk]",chunk["messages"][-1].content)
2.9.5 BaseStore ç»“æ„åŒ–å­˜å‚¨
æ˜¯langGraphæä¾›çš„é€šç”¨é”®å€¼å­˜å‚¨æŠ½è±¡æ¥å£ï¼Œä¸“ä¸ºç»“æ„åŒ–é•¿æœŸè®°å¿†æ¶‰åŠï¼Œæ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ï¼š
å‘½ä»¤ç©ºé—´æœºåˆ¶
å±‚æ¬¡åŒ–å…ƒç¥–è·¯å¾„ç»„ç»‡æ•°æ®ï¼Œç±»ä¼¼äºç³»ç»Ÿç›®å½•ç»“æ„

namespace = ("users","user_123")
å¯¹åº”é€»è¾‘è·¯å¾„ users/user_123

put(namespace, key, value)
get(namespace, key)
search(namespace, query)
delete(namespace, key)
def test4():
    from dataclasses import dataclass

    from langchain_core.runnables import RunnableConfig
    from langchain.agents import create_agent
    from langchain.tools import tool, ToolRuntime
    from langgraph.store.memory import InMemoryStore
    
    
    @dataclass
    class Context:
        user_id: str
    
    # InMemoryStore å°†æ•°æ®ä¿å­˜åˆ°å†…å­˜å­—å…¸ä¸­ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨åŸºäºæ•°æ®åº“çš„å­˜å‚¨ã€‚
    store = InMemoryStore() # [!code highlight]
    
    # ä½¿ç”¨ put æ–¹æ³•å‘ store å†™å…¥ç¤ºä¾‹æ•°æ®ï¼Œå¦‚æœKEYå­˜åœ¨ï¼Œç›´æ¥æ›´æ–°
    store.put( # [!code highlight]
        ("users",),  # ç”¨äºå°†ç›¸å…³æ•°æ®åˆ†ç»„çš„å‘½åç©ºé—´ï¼ˆç”¨äºç”¨æˆ·æ•°æ®çš„ users å‘½åç©ºé—´ï¼‰
        "user_123",  # å‘½åç©ºé—´å†…çš„ Keyï¼ˆç”¨æˆ· ID ä½œä¸º Keyï¼‰
        {
            "name": "John Smith",
            "language": "English",
        }  # ä¸ºç»™å®šç”¨æˆ·å­˜å‚¨çš„æ•°æ®
    )
    store.put( # [!code highlight]
        ("users", "preferences"),   # ç”¨äºå°†ç›¸å…³æ•°æ®åˆ†ç»„çš„å‘½åç©ºé—´ï¼ˆç”¨äºç”¨æˆ·æ•°æ®çš„ users å‘½åç©ºé—´ï¼‰
        "user_123",  # å‘½åç©ºé—´å†…çš„ Keyï¼ˆç”¨æˆ· ID ä½œä¸º Keyï¼‰
        {
            "name": "John Smith2",
            "language": "English",
        }  # ä¸ºç»™å®šç”¨æˆ·å­˜å‚¨çš„æ•°æ®
    )
    
    @tool
    def get_user_info(runtime: ToolRuntime[Context]) -> str:
        """Look up user info."""
        # è®¿é—® store - ä¸æä¾›ç»™ `create_agent` çš„ store ç›¸åŒ
        store = runtime.store # [!code highlight]
        user_id = runtime.context.user_id
        # ä» store æ£€ç´¢æ•°æ® - è¿”å›å¸¦æœ‰ value å’Œ metadata çš„ StoreValue å¯¹è±¡
        user_info = store.get(("users","preferences"), user_id) # [!code highlight]
        return str(user_info.value) if user_info else "Unknown user"
    
    tools = [get_user_info]
    
    agent = create_agent(
        model=model, 
        tools=[get_user_info],
        # å°† store ä¼ é€’ç»™æ™ºèƒ½ä½“ - ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨è¿è¡Œå·¥å…·æ—¶è®¿é—® store
        store=store, # [!code highlight]
        context_schema=Context
    )
    
    # è¿è¡Œæ™ºèƒ½ä½“
    agent.invoke(
        {"messages": [{"role": "user", "content": "å¸®æˆ‘æŸ¥çœ‹ç”¨æˆ·ä¿¡æ¯"}]},
        context=Context(user_id="user_123") # [!code highlight]
    )
from langgraph.graph.state import BaseStore

# è·¨çº¿ç¨‹éœ€è¦é€šè¿‡user_idæ¥ä¼ é€’ä¿¡æ¯ï¼Œé€šè¿‡è‡ªå®šä¹‰stateæ¥ä¼ é€’
from langchain.agents import create_agent,AgentState
class CustomState(AgentState):
    user_id:str

# å®šä¹‰ç”¨æˆ·ä¿¡æ¯æå–ç±»
from pydantic import BaseModel,Field
class UserInfo(BaseModel):
    '''ä»æ–‡æœ¬ä¸­æå–ç”¨æˆ·ä¿¡æ¯'''
    name:str = Field(description="ç”¨æˆ·å")
    info:str = Field(description="ç”¨æˆ·ä¿¡æ¯")

class QueryInfo(BaseModel):
    ''' ä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–ä¿¡æ¯'''
    name:str = Field(description="ç”¨æˆ·å, å¦‚æœæŸ¥è¯¢ä¸­åŒ…å«'æˆ‘'ç­‰ç¬¬ä¸€äººç§°ï¼Œéœ€è¦ä»å†å²å¯¹è¯ä¸­æå–ç”¨æˆ·åï¼Œå¦åˆ™è¿”å›'all_users'")
    query_content:str = Field(description="æŸ¥è¯¢å†…å®¹, ä¾‹å¦‚èŒä¸šå…´è¶£ç­‰")

# è®°å¿†ç®¡ç†å·¥å…·
from langchain_core.tools import tool
from langchain.tools import InjectedState,InjectedStore
from typing import Annotated
import uuid
from datetime import datetime
# stroreå‚æ•°ä½¿ç”¨Injected æ³¨è§£ï¼Œç”±langgraphè‡ªåŠ¨ä¼ å…¥
# InjectedStore() æ ‡æ—§ä¼šè®© Pydantic åœ¨ç”Ÿæˆ JSON Schema æ—¶è·³è¿‡è¿™ä¸ªå‚æ•°
# LLM ä¸ä¼šçœ‹åˆ° store åªä¼šçœ‹åˆ° user_id å’Œ info
@tool
def save_memory(
    info:str,
    state: Annotated[dict, InjectedState()],
    store: Annotated[BaseStore, InjectedStore()]
    ):
    '''å°†ç”¨æˆ·ä¿¡æ¯å­˜å‚¨åˆ°è®°å¿†ä¸­
        å·¥å…·ä¼šè‡ªåŠ¨ä»stateè·å–user_idï¼Œä½¿ç”¨pydanticæå–ç”¨æˆ·ä¿¡æ¯
    '''
    print("æ‰§è¡Œsave_memoryå·¥å…·", info)
    structed_llm = model.with_structured_output(UserInfo)
    try:
        extractedinfo = structed_llm.invoke(info)
        username = extractedinfo.name.lower()
        state_user_id = state.get("user_id")
        user_id = username if username and username != "unknown" else state_user_id
        full_info = f"{extractedinfo.name},{extractedinfo.info}"
    except Exception as e:
        # deepseek-chat ä¸æ”¯æŒ with_structured_output ç»“æ„åŒ–è¾“å‡º
        user_id = state.get("user_id","unknown_user")
        full_info = info

    namespace = (user_id, "profile")
    memory_id = str(uuid.uuid4())
    store.put(
        namespace,
        memory_id,
        {
            "info":full_info,
            "timestamp":datetime.now().isoformat(),
            "source":"user"
        }
    )
    print(f"å·²ä¿å­˜ç”¨æˆ·ä¿¡æ¯ {user_id} {full_info}")
    return f"å·²ä¿å­˜ç”¨æˆ·ä¿¡æ¯ {user_id} {full_info}"

@tool
def get_user_info(
        query:str,
        state: Annotated[dict, InjectedState()],
        store: Annotated[BaseStore, InjectedStore()]
    ):
    '''æ£€ç´¢ç”¨æˆ·ä¿¡æ¯ï¼Œæ­¤å·¥å…·ä¼šè‡ªåŠ¨ä»stateè·å–user_id, query æŸ¥è¯¢å…³é”®å­—ï¼Œæ¯”æ–¹å¼ ä¸‰'''
    print("æ‰§è¡Œget_user_infoå·¥å…·")
    state_user_id = state.get("user_id", None)
    print(f"state_user_id: {state_user_id}")
    if state_user_id != None:
        try:
            structed_llm = model.with_structured_output(QueryInfo)
            print(f"query: {query}")
            extractedinfo = structed_llm.invoke(query)
            print(f"extractedinfo: {extractedinfo}")
            if extractedinfo.name.lower() in ["all_users","current_users","unknown"]:
                user_id = None
            else:
                user_id = extractedinfo.name.lower()
        except:
            print(f"with_structured_output err")
            agent = create_agent(
                    system_prompt='''
                    åˆ†æä¸‹éœ€è¦æŸ¥çœ‹è°çš„ä¿¡æ¯,è¿”å›å§“åå³å¯
                    ''',
                model=model
            )
            res = agent.invoke({'messages':[HumanMessage(content=query)]})
            print(f"res: {res['messages'][-1].content}")
            user_id = "å¼ ä¸‰"
            
    else:
        print("state_user_id is None")
        user_id = None

    if user_id:
        namespace = (user_id, "profile")
        memory = store.search(namespace,limit=20)
    else:
        memory = []

    if not memory:
        print(f"æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·ä¿¡æ¯ {user_id}")
        return f"æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·ä¿¡æ¯ {user_id}"
    
    result = []
    for item in memory:
        info = item.value.get("info")
        timestamp = item.value.get("timestamp")
        # source = item.value.get("source")
        result.append(f"{info} {timestamp}")

    output = '\n'.join(result)
    print(f"æ‰¾åˆ°{len(result)}æ¡è®°å¿†: {output}")
    return f"æ‰¾åˆ°{len(result)}æ¡è®°å¿†: {output}"

from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.store.postgres import PostgresStore
from psycopg_pool import ConnectionPool
DB_URL = "postgresql://postgres:zkjiao@localhost:5432/postgres"

with ConnectionPool(conninfo=DB_URL,max_size=10,kwargs={"autocommit":True}) as pool:
    checkpointer = PostgresSaver(pool)
    store = PostgresStore(pool)

    checkpointer.setup()
    store.setup()

    agent = create_agent(
        checkpointer=checkpointer,
        state_schema=CustomState,
        store=store,
        system_prompt='''
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯å’Œä¿å­˜ç”¨æˆ·ä¿¡æ¯, è¯·æ˜ç¡®æè¿°ä½ è°ƒç”¨çš„å·¥å…·å’Œç»“æœã€‚ã€
        è®°å¾—ä½¿ç”¨save_memoryä¿å­˜ç”¨æˆ·ä¿¡æ¯ï¼Œä½¿ç”¨get_user_infoæŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯ã€‚
        ''',
        tools=[get_user_info,save_memory],
        model=model
    )

    config = {
        "configurable":{"thread_id":"user5"}
    }
    response = agent.invoke({
        "messages": [HumanMessage(content="æˆ‘æ˜¯å¼ ä¸‰ï¼Œæˆ‘çš„èŒä¸šæ˜¯ç¨‹åºå‘˜")],
        "user_id":"å¼ ä¸‰"
    }, config=config)
    print('[response]',response["messages"][-1].content)

    # response = agent.invoke({
    #     "messages": [HumanMessage(content="æˆ‘æ˜¯è°")],
    #     "user_id":"å¼ ä¸‰"
    # }, config=config)
    # print('[response]',response["messages"][-1].content)

    config = {
        "configurable":{"thread_id":"user11"}
    }
    response = agent.invoke({
        "messages": [HumanMessage(content="å¸®æˆ‘æŸ¥ä¸€ä¸‹å¼ ä¸‰çš„èŒä¸šä¿¡æ¯")],
        "user_id":"å¼ ä¸‰"
    }, config=config)
    print('[response]',response["messages"][-1].content)
# psql -U postgres -d postgres -c "SELECT * FROM checkpoints WHERE thread_id ='user_lisi' LIMIT 3;"
3 langgraph studioå›¾ç»“æ„ 
æœ¬åœ°éƒ¨ç½²ï¼Œè®¿é—®äº‘ç«¯ï¼Œå…ˆè·³è¿‡
4 langchainä¸­é—´ä»¶

after_model   æ—¥å¿—ç›‘æ§
before_model  å®‰å…¨è„±æ•ï¼Œå‚æ•°æ ¡éªŒï¼Œå¯¹è¯æ€»ç»“ï¼Œæ¶ˆæ¯é˜Ÿåˆ—
wrap_model_call  ç¼“å­˜ï¼Œé™æµï¼Œæ¨¡å‹é™çº§ï¼ˆåŠ¨æ€åˆ‡æ¢æ¨¡å‹ï¼‰
wrap_tool_call å·¥å…·é‡è¯•ï¼Œå·¥å…·å®¡è®¡ï¼Œå¤–éƒ¨APIè°ƒç”¨
before_agent  æ³¨å†Œ

åˆ†ç±»ï¼š
ç›‘æ§ç±»ï¼š
ä¿®æ”¹ç±»ï¼šä¸Šä¸‹æ–‡ç®¡ç†
æ§åˆ¶ç±»ï¼šæµç¨‹æ§åˆ¶ï¼Œäººå·¥ä»‹å…¥
å¼ºåˆ¶ç±»ï¼šå®‰å…¨åˆè§„ç­‰

ç”Ÿå‘½å‘¨æœŸ
before_agent
before_model
wrap_model_call
wrap_tool_call
after_model
after_agent


ç¤ºä¾‹
before_modelï¼š
SummarizationMiddleware ä¸Šä¸‹æ–‡å‹ç¼©ä¸­é—´ä»¶
PIIMiddleware ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ æ£€æµ‹å’Œå¤„ç†ä¸­é—´ä»¶
ModelCallLimitMiddleware  æ¨¡å‹è°ƒç”¨é™åˆ¶ä¸­é—´ä»¶æ¬¡æ•°


4.1 before_model
SummarizationMiddleware ä¸Šä¸‹æ–‡å‹ç¼©ä¸­é—´ä»¶
PIIMiddleware ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ æ£€æµ‹å’Œå¤„ç†ä¸­é—´ä»¶
ModelCallLimitMiddleware  æ¨¡å‹è°ƒç”¨é™åˆ¶ä¸­é—´ä»¶æ¬¡æ•°

# ä¸Šä¸‹æ–‡å‹ç¼©ä¸­é—´ä»¶
from langchain.agents.middleware import SummarizationMiddleware
from langchain.agents import create_agent
# ("messages", 10)     # ä¿ç•™æœ€è¿‘10æ¡åŸå§‹æ¶ˆæ¯
# ("tokens", 2000)     # ä¿ç•™æœ€å¤š2000 tokensçš„å†…å®¹
# ("fraction", 0.2)    # ä¿ç•™20%çš„ä¸Šä¸‹æ–‡å®¹é‡
summarization_middleware  = SummarizationMiddleware(
    model=model,                     
    trigger=[("messages", 8)],        # tokenè¾¾åˆ°3500æ—¶è§¦å‘,è§¦å‘æ¡ä»¶
    keep=("messages", 5),             # æ‘˜è¦åä¿ç•™15æ¡æœ€æ–°æ¶ˆæ¯ï¼Œè§¦å‘åä¿ç•™çš„ä¿¡æ¯
    # token_counter=count_tokens_accurately,  # ç²¾ç¡®è®¡æ•°
    summary_prompt="è¯·å°†ä»¥ä¸‹å¯¹è¯è¿›è¡Œæ‘˜è¦ï¼Œä¿ç•™å…³é”®å†³ç­–ç‚¹å’Œç»†èŠ‚..."  # è‡ªå®šä¹‰æç¤ºè¯
)

agent = create_agent(
    model=model,
    middleware=[summarization_middleware]
)

long_history = [HumanMessage(content=f"é—®é¢˜ {i}:å¦‚ä½•è¯„ä»·æŸé¡¹ä¸“åˆ©æŠ€æœ¯çš„é£é™©") for i in range(10)]
print(f"å»ºç«‹äº†å†å²å¯¹è¯ {len(long_history)} æ¡")

result = agent.invoke(
    {"messages": long_history}
)

messages = result.get("messages",[])
print(f"æ‘˜è¦åçš„å¯¹è¯ {len(messages)} æ¡")

if len(messages) <  len(long_history):
    print(f"ä¸­é—´é—´è§¦å‘ï¼Œè£å‰ªäº†{len(long_history) - len(messages)}æ¡å¯¹è¯")
# ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ æ£€æµ‹å’Œå¤„ç†ä¸­é—´ä»¶
# pii_type ç±»å‹ï¼šéœ€é…åˆ detector ä½¿ç”¨
# "email"         # é‚®ç®±åœ°å€ï¼šuser@example.com
# "credit_card"   # ä¿¡ç”¨å¡å·ï¼š4111-1111-1111-1111ï¼ˆå«Luhnç®—æ³•éªŒè¯ï¼‰
# "ip"            # IPåœ°å€ï¼š192.168.1.1
# "mac_address"   # MACåœ°å€ï¼š00:1A:2B:3C:4D:5E
# "url"           # URLé“¾æ¥ï¼šhttps://example.com
# strategy (å¤„ç†ç­–ç•¥)
# blockï¼šæ‹¦æˆªæ¨¡å¼, å‡ºç°PIIæ—¶ç›´æ¥æŠ›å‡ºå¼‚å¸¸
# redactï¼šå®Œå…¨æ›¿æ¢æ¨¡å¼ æˆ‘çš„é‚®ç®±æ˜¯ user@example.com-> æˆ‘çš„é‚®ç®±æ˜¯ [REDACTED_EMAIL]
# maskï¼šéƒ¨åˆ†é®ç›–æ¨¡å¼ ****-****-****-1234
# hashï¼šå“ˆå¸ŒåŒ–æ¨¡å¼ user@example.com-><email_hash:a1b2c3d4>

from langchain.agents.middleware import PIIMiddleware
from langchain.agents.middleware._redaction import PIIMatch
def detect_phone_numbers(text: str) -> list[PIIMatch]:
    import re
    matches = []
    for match in re.finditer(r"1[3-9]\d{9}", text):
        matches.append(PIIMatch(
            type="phone",
            start=match.start(),
            end=match.end(),
            value=match.group()
        ))
    return matches

# æ£€æµ‹å¹¶æ›¿æ¢æ‰€æœ‰é‚®ç®±
pii_middleware = PIIMiddleware(
    pii_type="phone",
    strategy="mask",  # æ›¿æ¢ä¸º[REDACTED_EMAIL]
    detector=detect_phone_numbers,  # è‡ªå®šä¹‰å‡½æ•°
    apply_to_input=True,
    apply_to_output=True  # AIå›å¤ä¹Ÿè¦æ£€æŸ¥
)

from langchain.agents import create_agent

# å¤šä¸ªä¸­é—´ä»¶ç»„åˆä½¿ç”¨
agent = create_agent(
    model=model,
    middleware=[
        # # ä¿¡ç”¨å¡ï¼šéƒ¨åˆ†é®ç›–ï¼ˆæ˜¾ç¤ºå4ä½ï¼‰
        # PIIMiddleware("credit_card", strategy="mask"),
        # # IPåœ°å€ï¼šå“ˆå¸ŒåŒ–ï¼ˆç”¨äºè°ƒè¯•è¿½è¸ªï¼‰
        # PIIMiddleware("ip", strategy="hash"),
        # # URLï¼šå®Œå…¨æ›¿æ¢
        # PIIMiddleware("url", strategy="redact"),
        pii_middleware
    ]
)

# ç”¨æˆ·è¾“å…¥åŒ…å«æ•æ„Ÿä¿¡æ¯
result = agent.invoke({
    "messages": "æˆ‘çš„ä¿¡ç”¨å¡æ˜¯ 4111-1111-1111-1111ï¼Œè¯·è®¿é—® http://bank.com æŸ¥çœ‹è´¦å•ï¼Œæˆ‘çš„ç”µè¯æ˜¯ï¼š 15956555666"
})

print(result.get("messages",[]))
# æ¨¡å‹è°ƒç”¨é™åˆ¶ä¸­é—´ä»¶æ¬¡æ•°
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents.middleware import ModelCallLimitMiddleware
from langchain.agents.middleware.model_call_limit import ModelCallLimitExceededError
# åˆ›å»ºæ£€æŸ¥ç‚¹å’Œä¸­é—´ä»¶
memory = InMemorySaver()
call_limiter = ModelCallLimitMiddleware(
    thread_limit=2,      # æ•´ä¸ªä¼šè¯æœ€å¤š2æ¬¡è°ƒç”¨
    exit_behavior="error"  # æ”¹ä¸ºerrorä¾¿äºæµ‹è¯•
    # exit_behavior="end"  # ä¼˜é›…ç»“æŸï¼Œä¸æŠ¥é”™
)

# åˆ›å»ºagentï¼Œå…³é”®ï¼šä¼ å…¥checkpointer
agent = create_agent(
    model,
    middleware=[call_limiter],
    checkpointer=memory  # å¿…é¡»ä¼ å…¥checkpointer
)

# ç”¨æˆ·å¯¹è¯
thread_id = "user_123"
print("=== æµ‹è¯•thread_limité™åˆ¶ ===")

for i in range(4):  # å°è¯•4æ¬¡
    try:
        # å…³é”®ï¼šconfigå¿…é¡»åŒ…å«thread_id
        result = agent.invoke(
            {"messages": f"ç®€å•é—®é¢˜{i}: ä½ å¥½å—ï¼Ÿ"},
            config={"configurable": {"thread_id": thread_id}}
        )
        print(f"è°ƒç”¨{i+1}: æˆåŠŸ")
    except ModelCallLimitExceededError as e:
        if "limit exceeded" in str(e).lower():
            print(f"è°ƒç”¨{i+1}: è¶…å‡ºé™åˆ¶ - {e}")
        else:
            print(f"è°ƒç”¨{i+1}: å…¶ä»–é”™è¯¯ - {e}")
4.2 wrap_model_call 
ContextEditingMiddleware ä¸Šä¸‹æ–‡ç¼–è¾‘ä¸­é—´ä»¶
ModelFallbackMiddleware æ¨¡å‹æ•…éšœè‡ªåŠ¨åˆ‡æ¢
LLMToolSelectorMiddleware æ™ºèƒ½å·¥å…·é€‰æ‹©
# ä¸Šä¸‹æ–‡ç¼–è¾‘ä¸­é—´ä»¶
from langchain.agents.middleware import ContextEditingMiddleware
from langchain.agents.middleware.context_editing import ClearToolUsesEdit
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.tools import tool

context_editor = ContextEditingMiddleware(
    edits=[ClearToolUsesEdit(
        trigger=10,          # 200K tokensæ‰è§¦å‘
        # clear_at_least=50,         # æ¸…ç†å¤šå°‘éƒ½å¯ä»¥
        keep=0,                  # ä¿ç•™æœ€è¿‘10ä¸ªå·¥å…·ç»“æœ
        # clear_tool_inputs=False,  # ä¿ç•™å·¥å…·è¾“å…¥
        # exclude_tools=[],         # ä¸æ’é™¤ä»»ä½•å·¥å…·
        placeholder="ğŸ’¥ CLEARED ğŸ’¥"   # ç®€æ´å ä½ç¬¦
    )],  # æ˜ç¡®æŒ‡å®šç¼–è¾‘ç­–ç•¥
    token_count_method="approximate"    # ä½¿ç”¨ç²¾ç¡®è®¡æ•°
)

# åˆ›å»ºæ£€æŸ¥ç‚¹å’Œä¸­é—´ä»¶
memory = InMemorySaver()

@tool
def get_data(city: str) -> str:
    """è¿™ä¸ªå‡½æ•°ä¼šè¿”å›10 tokençš„æ•°æ®"""
    data_lines = []
    for i in range(5):
        data_lines.append(f"{city}çš„ç¬¬{i+1}è¯¦ç»†æ•°æ®ï¼š" + "è¿™æ˜¯æ•°æ®å†…å®¹")
    return '\n'.join(data_lines)

# åˆ›å»ºagentï¼Œå…³é”®ï¼šä¼ å…¥checkpointer
agent = create_agent(
    model,
    tools=[get_data],
    middleware=[context_editor],
    checkpointer=memory,
    # debug=True
)

# ç”¨æˆ·å¯¹è¯
thread_id = "user_123"

for i in range(1):  # å°è¯•4æ¬¡
    # å…³é”®ï¼šconfigå¿…é¡»åŒ…å«thread_id
    print('')
    print('--------------------------------')
    print(f'è°ƒç”¨{i+1}')
    result = agent.invoke(
        {"messages": f"ç®€å•é—®é¢˜{i}: å¸®æˆ‘ä½¿ç”¨get_dataå·¥å…·æŸ¥è¯¢ä¸€ä¸‹æ¸©å·çš„æ•°æ®ã€‚"},
        config={"configurable": {"thread_id": thread_id}}
    )
       # æ£€æŸ¥æ¶ˆæ¯åˆ—è¡¨
    if "messages" in result:
        print(f"æ¶ˆæ¯æ•°é‡: {len(result['messages'])}")
        
        # æ‰“å°æ¯æ¡æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯
        for j, msg in enumerate(result["messages"]):
            msg_type = type(msg).__name__
            print(f"\næ¶ˆæ¯{j} ({msg_type}):")
            
            # æ‰“å°å†…å®¹
            if hasattr(msg, 'content'):
                content_preview = msg.content[:200] + "..." if len(msg.content) > 200 else msg.content
                print(f"å†…å®¹: {content_preview}")

from langchain.agents.middleware import ModelFallbackMiddleware
from langchain.agents.middleware import LLMToolSelectorMiddleware
4.3 wrap_tool_call
ToolRetryMiddleware é‡è¯•
LLMToolEmulator æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œï¼Œæ‰§è¡ŒæŸäº›å·¥å…·ä¸ä¼šçœŸå®è°ƒç”¨ï¼Œè€Œæ˜¯æ¨¡æ‹Ÿçš„ï¼Œä¸»è¦æ˜¯ç”¨äºé«˜é£é™©åœºæ™¯å·¥å…·æ¨¡æ‹Ÿ
4.4 after_model
HumanInTheLoopMiddleware äººå·¥å¹²é¢„ä¸­é—´ä»¶
ToolCallLimitMiddleware å·¥å…·è°ƒç”¨é™åˆ¶
5. è‡ªå®šä¹‰ä¸­é—´ä»¶

ä¸€äº›æ¦‚å¿µ
ModelRequest, ModelResponse å•è¯è°ƒç”¨çº§åˆ«çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹ï¼Œé€šè¿‡ç›´æ¥ä¿®æ”¹request.modelæˆ–è€…toolå¯ä»¥å®ç°åŠ¨æ€è·¯ç”±æˆ–åŠ¨æ€å·¥å…·ï¼Œmessagesæ¶ˆæ¯åˆ—è¡¨ï¼Œruntime è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ŒstateçŠ¶æ€
handlerï¼šæ‰§è¡Œå™¨å‡½æ•°ï¼Œå…¥å‚ModelRequestï¼Œå‡ºé¤ModelResponseï¼Œä½¿ç”¨handler(request)ï¼Œè¿›è¡Œè°ƒç”¨,æœ¬èº«ä¸å¯ä¿®æ”¹
AgentStateï¼šå…¨å±€å®¹å™¨ï¼Œç”Ÿå‘½å‘¨æœŸæ—¶æ•´ä¸ªAIçš„æ‰§è¡Œæµç¨‹ ã€‚åœ¨before_model,after_model,before_agent ç­‰èŠ‚ç‚¹é’©å­éœ€è¦è·¨å¤šæ¬¡è°ƒç”¨ è·å–æŒä¹…åŒ–æ•°æ®ï¼Œæˆ–è€…è®¿é—®å…¨å±€å¯¹è¯å†å²
Command æŒ‡ä»¤è¯­è¨€ï¼Œå‘Šè¯‰æ¡†æ¶ï¼šâ€œä¸è¦æŒ‰æ­£å¸¸æµç¨‹èµ°ï¼Œè€Œæ˜¯æ‰§è¡Œè¿™ä¸ªç‰¹æ®Šæ“ä½œâ€ã€‚

tips:
åœ¨wrap_model_call å’Œ wrap_tool_callè¿™ç§éå¯è§†èŠ‚ç‚¹ä¸­å¯ä»¥å¯¹requestè¿›è¡Œèµ‹å€¼ä¿®æ”¹ï¼Œä¸èƒ½ä¿®æ”¹config
åœ¨å…¶ä»–å¯è§†èŠ‚ç‚¹ä¸­ï¼Œåªèƒ½è¯»å–stateï¼Œä¸èƒ½è®¿é—®request
def hook_config(
    *,
    can_jump_to: list[JumpTo] | None = None,
) -> Callable[[CallableT], CallableT]: 
è£…é¥°å™¨ï¼Œç±»ä¼¼äºCommand
Command

@before_model

def quota_checker_ middleware(state: AgentState):
    if state.get("total_tokens", 0) > 1_000_000:
        # ç›´æ¥è·³è½¬åˆ°ç»“æŸèŠ‚ç‚¹ï¼Œè·³è¿‡åç»­æ‰€æœ‰ä¸­é—´ä»¶å’Œæ¨¡å‹è°ƒç”¨
        return Command(
            jump_to="end", # ç›®æ ‡èŠ‚ç‚¹åç§°
            update = {"error": "Token é…é¢è€—å°½"}
        )

å¦‚ä½•ç†è§£è¿™ä¸ªcommand
        
1. jump_to="end" # åç»­ä¼šè®²èŠ‚ç‚¹åç§°æ˜¯ä»€ä¹ˆ
ä½œç”¨ï¼šä¸­æ–­å½“å‰æ‰§è¡Œæµç¨‹ï¼Œç›´æ¥è·³è½¬åˆ°åä¸º "end" çš„èŠ‚ç‚¹

æ•ˆæœï¼š

âœ… è·³è¿‡æ‰€æœ‰åç»­çš„ @before_model ä¸­é—´ä»¶

âœ… è·³è¿‡æ¨¡å‹è°ƒç”¨ï¼ˆä¸è°ƒç”¨ LLMï¼‰

âœ… è·³è¿‡æ‰€æœ‰ @after_model ä¸­é—´ä»¶

âœ… ç›´æ¥å¼€å§‹æ‰§è¡Œ "end" èŠ‚ç‚¹çš„å¤„ç†é€»è¾‘

2. update å‚æ•°
ä½œç”¨ï¼šæ›´æ–° AgentState çš„çŠ¶æ€æ•°æ®

æ•ˆæœï¼šåœ¨æ‰§è¡Œè·³è½¬å‰ï¼Œå°† {"error": "Token é…é¢è€—å°½"} åˆå¹¶åˆ°çŠ¶æ€ä¸­

# åŠ¨æ€æç¤ºè¯å‡½æ•°
from langchain.agents.middleware import ModelRequest, ModelResponse
from langchain.agents.middleware import dynamic_prompt
from typing import TypedDict

@tool
def get_weather(city: str) -> str:
    """è·å–åœ°ç‚¹å¤©æ°”ä¿¡æ¯"""
    return f'{city}çš„å¤©æ°”æ˜¯ï¼šæ™´25åº¦'

class Context(TypedDict):
    user_role: str # è§’è‰²

@dynamic_prompt
def role_base_peompt(request: ModelRequest) -> str:
    user_role = request.runtime.context.get("user_role")
    # print(user_role)
    if user_role == "expert":
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ°”è±¡åˆ†æå¸ˆ, è¯·æä¾›è¯¦ç»†æ•°æ®"""
    elif user_role == "user":
        return f"""ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„å¯¼æ¸¸, è¯·æä¾›è¯¦ç»†æ•°æ®"""
    else:
        return f"""ä½ æ˜¯ä¸€ä¸ªå¤©æ°”åŠ©æ‰‹"""

agent = create_agent(
    model=model,
    tools=[get_weather],
    middleware=[role_base_peompt],
    context_schema=Context,
    )

print("****ä¸“å®¶")
result = agent.invoke({"messages": [HumanMessage(content="åŒ—äº¬å¤©æ°”")]},context={"user_role": "expert"})
print(result['messages'][-1].content)

print("****å¯¼æ¸¸")
result = agent.invoke({"messages": [HumanMessage(content="åŒ—äº¬å¤©æ°”")]},context={"user_role": "user"})
print(result['messages'][-1].content)
# åŠ¨æ€åˆ‡æ¢å¤§æ¨¡å‹
from langchain.agents.middleware import wrap_model_call
from typing import Callable

@wrap_model_call
async def dynamic_model_call(request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:
    state = request.state
    messages = state.get("messages", [])

    request.override(model=model)
    return await handler(request)
from langchain.agents.middleware import AgentMiddleware, AgentState
from langchain.agents import AgentRuntime
# ä½¿ç”¨åŸç”Ÿæ–¹å¼ ä¼šè¯æŒä¹…åŒ–å­˜å‚¨
class PersistentMiddleware(AgentMiddleware):
    def __init__(self,path:str) -> None:
        super().__init__()
        self.path = path

    def after_model(self, state: AgentState, runtime: AgentRuntime) -> AgentState:
        messages = state.get("messages", [])
        pass

def persistent(path):
    return PersistentMiddleware(path)

agent = create_agent(
    model=model,
    tools=[],
    middleware=[persistent("test.json")],
    # context_schema=Context,
    )
6.RAG
6.1 RAG
# æ–‡æ¡£åŠ è½½
from langchain_community.document_loaders import TextLoader, Docx2txtLoader
loader = TextLoader("test.txt", encoding="utf-8")
docs = loader.load()
# print(docs[0].page_content)

# æ–‡æ¡£åˆ‡åˆ†
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # å—å¤§å°
    chunk_overlap=20, # é‡å åŒºé—´
    separators=["\n\n", "\n"]) # åˆ†éš”ç¬¦ï¼šæ·»åŠ  "\n# " è®©æ ‡é¢˜å•ç‹¬æˆå—
texts = text_splitter.split_documents(docs)

# è¿‡æ»¤æ‰ç©ºæ–‡æ¡£å’ŒåªåŒ…å«åˆ†éš”ç¬¦çš„æ–‡æ¡£
filtered_texts = []
for text in texts:
    content = text.page_content.strip()
    # è¿‡æ»¤æ‰ç©ºå†…å®¹å’ŒåªåŒ…å« # çš„å†…å®¹
    if content and not content.strip() == "":
        filtered_texts.append(text)
texts = filtered_texts
# print(texts)
print(f"æ–‡æ¡£åˆ‡åˆ†å®Œæˆï¼Œå…± {len(texts)} ä¸ªæœ‰æ•ˆæ–‡æ¡£å—")

# æ–‡æ¡£çš„å‘é‡å­˜å‚¨åŠæ£€ç´¢
from langchain_community.vectorstores import FAISS
vectore_store = FAISS.from_documents(texts, simple_embedder)
vectore_store.save_local("faiss_index")

vectore = FAISS.load_local(
    "faiss_index",
    simple_embedder,
    allow_dangerous_deserialization=True, # è¿™ä¸ªæ˜¯å¿…é¡»è¦å…è®¸ååºåˆ—åŒ–ï¼Œå¦åˆ™ä¼šæŠ¥é”™
)

print("å‘é‡æ•°æ®åº“åˆ›å»ºåŠä¿å­˜")

# åŠ è½½å¹¶åˆ›å»ºæ£€ç´¢å™¨
# BM25Retriever åŸºäºå…³é”®è¯åŒ¹é…çš„æ£€ç´¢å™¨
# EnsembleRetriever åŸºäºå¤šä¸ªæ£€ç´¢å™¨çš„ç»„åˆæ£€ç´¢å™¨
from langchain_core.retrievers import BaseRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
# from langchain.retrievers.ensemble import EnsembleRetriever
# from langchain_community.retrievers import ContextualCompressionRetriever
import hashlib

class CustomEnsembleRetriever(BaseRetriever):
    """æ­£ç¡®çš„ BaseRetriever ç»§æ‰¿å®ç°"""
    
    retrievers: List[BaseRetriever]  # å¿…é¡»å£°æ˜ä¸ºç±»å±æ€§
    weights: List[float] = None
    k: int = 3
    
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    def __init__(self, retrievers: List[BaseRetriever], weights: List[float] = None, k: int = 3, **kwargs):
        # ä½¿ç”¨ super().__init__() æ¥åˆå§‹åŒ– BaseRetriever
        super().__init__(retrievers=retrievers, weights=weights, k=k, **kwargs)
        
    def _get_relevant_documents(
        self, query: str, *, run_manager: Any = None
    ) -> List[Document]:
        """æ ¸å¿ƒæ£€ç´¢æ–¹æ³•"""
        all_docs = []
        
        # ä¸ºæ¯ä¸ªæ£€ç´¢å™¨åˆ†é…æƒé‡
        weights = self.weights if self.weights else [1.0] * len(self.retrievers)
        
        for retriever, weight in zip(self.retrievers, weights):
            try:
                docs = retriever.invoke(query)
                # ä¸ºæ–‡æ¡£æ·»åŠ æƒé‡ä¿¡æ¯
                for doc in docs:
                    doc.metadata["ensemble_weight"] = weight
                    doc.metadata["retriever_type"] = retriever.__class__.__name__
                all_docs.extend(docs)
            except Exception as e:
                print(f"Warning: {retriever.__class__.__name__} failed: {e}")
                continue
        
        # ä½¿ç”¨ RRF (Reciprocal Rank Fusion) ç®—æ³•èåˆç»“æœ
        return self._rrf_fusion(all_docs)
    
    def _rrf_fusion(self, all_docs: List[Document]) -> List[Document]:
        """RRF èåˆç®—æ³•"""
        # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆå”¯ä¸€ID
        doc_scores = {}
        
        for rank, doc in enumerate(all_docs):
            doc_id = self._get_doc_id(doc)
            weight = doc.metadata.get("ensemble_weight", 1.0)
            score = weight / (rank + 60)  # RRF å…¬å¼
            
            if doc_id in doc_scores:
                doc_scores[doc_id]["score"] += score
            else:
                doc_scores[doc_id] = {
                    "doc": doc,
                    "score": score
                }
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_items = sorted(
            doc_scores.values(),
            key=lambda x: x["score"],
            reverse=True
        )
        
        # è¿”å›å‰ k ä¸ªæ–‡æ¡£
        return [item["doc"] for item in sorted_items[:self.k]]
    
    def _get_doc_id(self, doc: Document) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦"""
        # ä½¿ç”¨å†…å®¹å’Œå…ƒæ•°æ®ç”Ÿæˆå“ˆå¸Œ
        content = doc.page_content[:500]
        metadata = str(sorted(doc.metadata.items()))
        combined = content + metadata
        
        return hashlib.md5(combined.encode()).hexdigest()
    
    def invoke(self, query: str, config: Optional[Dict[str, Any]] = None, **kwargs) -> List[Document]:
        """è°ƒç”¨æ¥å£ï¼ˆå…¼å®¹æ€§ï¼‰"""
        return self._get_relevant_documents(query)

#åˆ›å»ºBM25æ£€ç´¢å™¨
BM25_retriever = BM25Retriever.from_documents(texts)
BM25_retriever.k = 3

# å‘é‡æ•°æ®åº“æ£€ç´¢å…¶
faiss_retriever = vectore_store.as_retriever(
    search_kwargs={"k": 3},
    search_type="similarity",
)

# æ–°åˆ›å»ºæ··åˆæ£€ç´¢å™¨
ensembled_retriever = CustomEnsembleRetriever(
    retrievers=[BM25_retriever, faiss_retriever],
    weights=[0.5, 0.5],
    k=3
)


from langchain_core.prompts import ChatPromptTemplate
# åŸºç¡€RAGæ­å»º
def format_docs(docs):
    return "\n\n".join([doc.page_content for doc in docs])


template = '''
ä½ æ˜¯ä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‘Šè¯‰ç”¨æˆ·ã€‚ç¦æ­¢ç¼–é€ ç­”æ¡ˆã€‚
ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}
å›ç­”ï¼š
'''

prompt = ChatPromptTemplate.from_template(template)

chain = ensembled_retriever | format_docs

retrieval = chain.invoke("åˆå§‹åŒ–ä»“åº“ä¿¡æ¯")
print("=====")
print(type(retrieval))
print(retrieval)

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
retrieval_chain = (
    {"context": ensembled_retriever | format_docs, "question": RunnablePassthrough()} 
    | prompt
    | model
    # | StrOutputParser()
)

content = retrieval_chain.invoke("åˆå§‹åŒ–ä»“åº“ä¿¡æ¯")
print(content)
6.2 agentic RAG https://appze9inzwc2314.xet-pc.citv.cn/p/t_pc/course_pc_detail/video/v_693aa957e4b0694ca1567036?anonymous=2&product_id=course_35xdCwcC9K8KTB5UwsSdormIVLt&type=6 å®æˆ˜
7.skill

7.1 åœ¨cursorä¸­éƒ¨ç½²
npm install -g openskills 
openskills install anthropics/skills 
openskills sync 

æ³¨æ„ï¼šå»ºè®®æŒ‰éœ€æ·»åŠ skillsï¼Œå› ä¸ºæ‰€æœ‰çš„skilléƒ½ä¼šæ¶ˆè€—ä¸€å®šçš„tokenï¼Œç±»ä¼¼äºrulesï¼Œä¸è¿‡skillsæ˜¯æ¸è¿›å¼çš„
from typing import Annotated, List, Optional, Dict, Any
from langchain.agents.middleware import AgentMiddleware
from langchain.agents.middleware import ModelRequest, ModelResponse
from langgraph.prebuilt.tool_node import ToolCallRequest
from langchain.agents import create_agent
from typing import Callable
class LoggingMiddleware(AgentMiddleware):
    def __init__(self, name: str):
        super().__init__()
        self.name = name
        self.call_count = 0

    def wrap_model_call(self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:
        self.call_count += 1
        print("*"*10)
        print(f"ç¬¬{self.call_count}æ¬¡è°ƒç”¨")
        print(request)
        if hasattr(request, "tools") and request.tools:
            tools = [t.name for t in request.tools]
            print(f"å¯ç”¨å·¥å…·{len(tools)}ä¸ª: {tools}")

        if hasattr(request, "state") and request.state:
            print(f"å½“å‰çŠ¶æ€ {request.state}")

        response = handler(request)
        print(f"æ¨¡å‹è°ƒç”¨å®Œæˆ")
        print()
        return response


# å‚è€ƒskillsï¼Œè¿½è¸ªä¸€ä¸ªå…³é”®çŠ¶æ€ skillsâ€”â€”loaded å½“å‰å·²åŠ è½½çš„æŠ€èƒ½åˆ—è¡¨

from langgraph.graph import MessagesState

# æ›¿æ¢æ¨¡å¼
# def skill_list_reducer(skills: List[str]) -> List[str]:
#     return skills
# class SkillState(MessagesState):
#     '''skill çŠ¶æ€schema MessagesStateåŒ…å«äº†messageå­—æ®µï¼Œåªéœ€è¦æ·»åŠ skills_loadedå­—æ®µ'''
#     skills_loaded: Annotated[List[str], skill_list_reducer] = []

# ç´¯è®¡æ¨¡å¼
def skill_list_accumulator(skills: List[str], new:List[str]) -> List[str]:
    if not skills: return new
    combine = skills + [i for i in new if i not in skills]
    return combine
class SkillState(MessagesState):
    skills_loaded: Annotated[List[str], skill_list_accumulator] = []

# loaderå·¥å…·
from langgraph.types import Command
from langchain_core.messages import ToolMessage
from langchain.tools import tool, ToolRuntime

@tool
def skill_data_analysis(runtime: ToolRuntime) -> Command:
    '''
    åŠ è½½æ•°æ®åˆ†ææŠ€èƒ½
    '''
    instruction = '''æ•°æ®åˆ†ææŠ€èƒ½å·²æˆåŠŸåŠ è½½ï¼Œç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·
    Â· calculate_statistics(numbers: List[float]) -> float: è®¡ç®—ä¸€ç»„æ•°å­—çš„ç»Ÿè®¡æ•°æ®
    
    è¯·ç»§ç»­ä½¿ç”¨ä¸€ä¸‹å·¥å…·å®Œæˆç”¨æˆ·çš„æ•°æ®åˆ†æä»»åŠ¡'''

    print("åŠ è½½æ•°æ®åˆ†ææŠ€èƒ½")
    return Command(
        update={
            "messages": [ToolMessage(
                content=instruction,
                tool_call_id=runtime.tool_call_id,
            )],
            "skills_loaded": ["data_analysis"]
        }
    )

@tool
def calculate_statistics(numbers: List[float]) -> float:
    '''è®¡ç®—ä¸€ç»„æ•°æ®çš„ç‰¹å¾å€¼'''
    if not numbers: return 'å…¥å‚ä¸ºç©º'
    return f"æ•°æ®é›†çš„ç»Ÿè®¡ç‰¹å¾å€¼: æ€»å’Œ: {sum(numbers)}, å¹³å‡å€¼: {sum(numbers)/len(numbers)}, æœ€å¤§å€¼: {max(numbers)}, æœ€å°å€¼: {min(numbers)}"

skills = [skill_data_analysis]
data_analysis = [calculate_statistics]
all_tools = skills + data_analysis

skills_mapping = {
    "data_analysis":data_analysis,
}

from langchain.tools import BaseTool
def get_tools_for_skills(input_skills: List[str]) -> List[BaseTool]:
    tools = list(skills)
    for skill in input_skills:
        if skill in skills_mapping:
            tools.extend(skills_mapping[skill])
    return tools

class SkillMiddleware(AgentMiddleware):
    '''
    Skill ä¸­é—´ä»¶ - å®ç°åŠ¨æ€å·¥å…·è¿‡æ»¤
    è¿™æ˜¯ Claude Skills çš„æ ¸å¿ƒç»„ä»¶!
    å·¥ä½œåŸç†ï¼š
    1. åœ¨æ¯æ¬¡æ¨¡å‹è°ƒç”¨å‰æ‹¦æˆªè¯·æ±‚
    2. ä» request.state ä¸­è¯»å– skills_loaded åˆ—è¡¨
    3. æ ¹æ® skills_loaded è¿‡æ»¤å·¥å…·åˆ—è¡¨
    4. ä½¿ç”¨ request.override() æ›¿æ¢å·¥å…·åˆ—è¡¨
    5. ä¼ é€’ç»™ä¸‹ä¸€ä¸ª handler
    è¿™æ ·ï¼Œæ¨¡å‹åœ¨æ¯æ¬¡è°ƒç”¨æ—¶åªä¼šçœ‹åˆ°ç›¸å…³çš„å·¥å…·ï¼
    '''
    def __init__(self, verbose = True):
        super().__init__()
        self.verbose = verbose
        self.call_count = 0
    
    def _get_skills_from_state(self, request: ModelRequest) -> List[str]:
        skills_loaded = []

        if hasattr(request, "state") and request.state:
            state = request.state
            # å­—å…¸ä¸èƒ½.æ–¹å¼è·å–å€¼ï¼Œéœ€è¦ä½¿ç”¨å­—å…¸æ–¹å¼è®¿é—®
            # if hasattr(state, "skills_loaded") and state.skills_loaded:
            #     skills_loaded = state.skills_loaded
            
            if isinstance(state, dict):
                skills_loaded = state.get("skills_loaded", [])
            elif hasattr(state, "skills_loaded"):
                skills_loaded = state.skills_loaded if state.skills_loaded else []

        return skills_loaded
    def wrap_model_call(self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:
        self.call_count += 1
        skills_loaded = self._get_skills_from_state(request)
        tools = get_tools_for_skills(skills_loaded)
        # request = request.override(tools=[tools])
        if self.verbose:
            print("-"*10)
            print(f"[SkillMiddleware]ç¬¬{self.call_count}æ¬¡è°ƒç”¨")
            print(f"å¯ç”¨å·¥å…·{len(tools)}ä¸ª: {tools}")
            print(f"å½“å‰çŠ¶æ€ {skills_loaded}")
            print(f"æ¨¡å‹è°ƒç”¨å®Œæˆ")
            print()
        return handler(request)

    def wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        """æ‹¦æˆªå·¥å…·è°ƒç”¨ï¼Œæ‰“å°å·¥å…·åç§°"""
        # ä¼˜å…ˆä½¿ç”¨ request.tool.nameï¼Œå¦åˆ™ä½¿ç”¨ request.tool_call["name"]
        if request.tool:
            tool_name = request.tool.name
        elif hasattr(request, 'tool_call') and isinstance(request.tool_call, dict):
            tool_name = request.tool_call.get("name", "unknown")
        else:
            tool_name = "unknown"
        print(f"ä½¿ç”¨äº†å·¥å…·: {tool_name}")
        return handler(request)


skill_middleware = SkillMiddleware(verbose=True)
skill_agent = create_agent(
    model=model,
    tools=all_tools,
    state_schema=SkillState,
    middleware=[skill_middleware],
)

test_input = {
    # "messages": [HumanMessage(content="å¦‚æœæˆ‘è®©ä½ è®¡ç®—1,2,3,4,5çš„å¹³å‡å€¼ï¼Œä½ æœ‰é‚£äº›å·¥å…·å¯ä»¥ä½¿ç”¨ï¼Œè¯·å‘Šè¯‰æˆ‘å·¥å…·ä¿¡æ¯")],
    "messages": [HumanMessage(content="è®¡ç®—1,2,3,4,5çš„å¹³å‡å€¼")],
    "skills_loaded":[]
}

res = skill_agent.invoke(test_input)
for i in res["messages"]:
    print("="*8)
    print(i.type, i.content)

# print(get_tools_for_skills(["skill_data_analysis"]))

ã€å®æˆ˜ã€‘Claude Skillså¹³å°æœ¬åœ°éƒ¨ç½² https://appze9inzwc2314.xet-pc.citv.cn/p/t_pc/course_pc_detail/video/v_694e7a67e4b0694ca160f64b?product_id=course_35xdCwcC9K8KTB5UwsSdormIVLt è¯¾ä»¶å­¦ä¹ 
8.langsmith

ç”³è¯·API: https://smith.langchain.com/o/43076bcb-6c39-4b00-8f55-9b38266c9a25/settings/apikeys

9.deepagent
ç³»ç»Ÿæç¤ºè¯ä¸å­ä»£ç†subagentå·¥å…·

å­æ™ºèƒ½ä½“æç¤ºè¯ä¸­é—´ä»¶ï¼šdeepagents\middleware

subagents é»˜è®¤æœ‰ä¸ªä¸€ä¸ªå­ä»£ç†å·¥å…·å task ï¼Œéœ€è¦æ—¶ä¼šä½¿ç”¨taskå·¥å…·äº§ç”Ÿå¤šä¸ªå­agentåŒæ—¶æ‰§è¡Œä»»åŠ¡
# æµç¨‹
from deepagents import create_deep_agent
from langchain_tavily import TavilySearch
from langgraph.checkpoint.memory import InMemorySaver
from langchain_community.utilities import SerpAPIWrapper
from langchain_core.tools import StructuredTool

serpapi_api_key = os.getenv("SERPAPI_API_KEY")
if not serpapi_api_key:
    print("è­¦å‘Š: SERPAPI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæœç´¢åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")

serpapi_wrapper = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)

# åˆ›å»ºåŒ…è£…å‡½æ•°ä»¥å¤„ç†é”™è¯¯
def google_search_func(query: str) -> str:
    """ä½¿ç”¨ Google æœç´¢å¼•æ“æœç´¢äº’è”ç½‘ä¿¡æ¯"""
    try:
        return serpapi_wrapper.run(query)
    except Exception as e:
        return f"æœç´¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}ã€‚è¯·æ£€æŸ¥ SERPAPI_API_KEY æ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚"

# å°†åŒ…è£…å‡½æ•°è½¬æ¢ä¸ºå·¥å…·
google_search = StructuredTool.from_function(
    func=google_search_func,
    name="google_search",
    description="ä½¿ç”¨ Google æœç´¢å¼•æ“æœç´¢äº’è”ç½‘ä¿¡æ¯ã€‚è¾“å…¥åº”è¯¥æ˜¯æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚å¦‚æœæœç´¢å¤±è´¥ï¼Œä¼šè¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ã€‚"
)

def record(content: str):
    with open("record.md", "w", encoding="utf-8") as f:
        f.write(content)

record = StructuredTool.from_function(
    func=record,
    name="record",
    description="å†™å…¥æœ¬åœ°æ–‡ä»¶å·¥å…·"
)

# 2. ç¼–å†™ç³»ç»Ÿæç¤ºè¯
research_instructions ='''
æ‚¨æ˜¯ä¸€ä½èµ„æ·±çš„ç ”ç©¶äººå‘˜ã€‚æ‚¨çš„å·¥ä½œæ˜¯è¿›è¡Œæ·±å…¥çš„ç ”ç©¶ï¼Œç„¶åæ’°å†™ä¸€ä»½ç²¾ç¾çš„æŠ¥å‘Šã€‚
æ‚¨å¯ä»¥é€šè¿‡äº’è”ç½‘æœç´¢å¼•æ“ä½œä¸ºä¸»è¦çš„ä¿¡æ¯æ”¶é›†å·¥å…·ã€‚

## å¯ç”¨å·¥å…·
### äº’è”ç½‘æœç´¢
ä½¿ç”¨æ­¤åŠŸèƒ½é’ˆå¯¹ç»™å®šçš„æŸ¥è¯¢è¿›è¡Œäº’è”ç½‘æœç´¢ã€‚æ‚¨å¯ä»¥æŒ‡å®šè¦è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡ã€ä¸»é¢˜ä»¥åŠæ˜¯å¦åŒ…å«åŸå§‹å†…å®¹ã€‚
### å†™å…¥æœ¬åœ°æ–‡ä»¶
ä½¿ç”¨æ­¤åŠŸèƒ½å°†ç ”ç©¶æŠ¥å‘Šä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ã€‚å½“æ‚¨å®Œæˆç ”ç©¶å¹¶ç”ŸæˆæŠ¥å‘Šåï¼Œè¯·ä½¿ç”¨æ­¤å·¥å…·å°†å®Œæ•´çš„æŠ¥å‘Šå†…å®¹ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚
- æ–‡ä»¶è·¯å¾„å»ºè®®ä½¿ç”¨ .md æ ¼å¼ï¼ˆMarkdownï¼‰ï¼Œä¾‹å¦‚ "research_report.md" æˆ– "./reports/æŠ¥å‘Šåç§°.md"
- è¯·ç¡®ä¿æŠ¥å‘Šå†…å®¹å®Œæ•´ã€ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«æ‰€æœ‰ç« èŠ‚å’Œå¼•ç”¨æ¥æº
### å·¥ä½œæµç¨‹
åœ¨è¿›è¡Œç ”ç©¶æ—¶ï¼š
1. é¦–å…ˆå°†ç ”ç©¶ä»»åŠ¡åˆ†è§£ä¸ºæ¸…æ™°çš„æ­¥éª¤
2. ä½¿ç”¨äº’è”ç½‘æœç´¢æ¥æ”¶é›†å…¨é¢çš„ä¿¡æ¯
3. å°†ä¿¡æ¯æ•´åˆæˆä¸€ä»½ç»“æ„æ¸…æ™°çš„æŠ¥å‘Š
4. **é‡è¦**ï¼šå®ŒæˆæŠ¥å‘Šåï¼ŒåŠ¡å¿…ä½¿ç”¨ `å†™å…¥æœ¬åœ°æ–‡ä»¶` å·¥å…·å°†å®Œæ•´æŠ¥å‘Šä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
5. åŠ¡å¿…å¼•ç”¨ä½ çš„èµ„æ–™æ¥æº
### æ³¨æ„ï¼šè¯·ç¡®ä¿åœ¨å®Œæˆç ”ç©¶åï¼Œå°†å®Œæ•´çš„æŠ¥å‘Šå†…å®¹ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°æŸ¥çœ‹å’Œä¿å­˜æŠ¥å‘Šã€‚
'''


agent = create_deep_agent(
    name = "deep_agent",
    model=model,
    tools=[
        # TavilySearch(max_results=3), 
        google_search, record],
    system_prompt=research_instructions,
    checkpointer=InMemorySaver(),
)

# agent = create_agent(
#     # name = "deep_agent",
#     model=model,
#     tools=[google_search],
#     # system_prompt=research_instructions,
#     checkpointer=InMemorySaver(),
# )

config = {"configurable": {"thread_id": "123"}}
for event in agent.stream(
    {
        "messages": [
            HumanMessage(content="å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´è¿™å®¶å…¬å¸"),
        ],
        
    },
    config=config
):
    print(event)
# print(result["messages"][-1].content)
{'PatchToolCallsMiddleware.before_agent': {'messages': Overwrite(value=[HumanMessage(content='å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´è¿™å®¶å…¬å¸', additional_kwargs={}, response_metadata={}, id='5056a728-8349-4f1a-ba13-a67d3cd0c46a')])}}
{'SummarizationMiddleware.before_model': None}
{'model': {'messages': [AIMessage(content='æˆ‘æ¥å¸®æ‚¨æŸ¥è¯¢å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´è¿™å®¶å…¬å¸çš„ç›¸å…³ä¿¡æ¯ã€‚è®©æˆ‘å…ˆåˆ¶å®šä¸€
ä¸ªç ”ç©¶è®¡åˆ’ï¼Œç„¶åè¿›è¡Œå…¨é¢çš„æœç´¢ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 5403, 'total_tokens': 5565, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 5376}, 'prompt_cache_hit_tokens': 5376, 'prompt_cache_miss_tokens': 27}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': '6fed0456-e381-4104-a61f-1e66c6dd86e1', 'finish_reason': 'tool_calls', 'logprobs': None}, name='deep_agent', id='lc_run--019c120f-0717-7472-8227-a15619ae8007-0', tool_calls=[{'name': 'write_todos', 'args': {'todos': [{'content': 'æœç´¢å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´å…¬å¸çš„åŸºæœ¬ä¿¡æ¯', 'status': 'in_progress'}, {'content': 'æŸ¥è¯¢å…¬å¸çš„å·¥å•†æ³¨å†Œä¿¡æ¯', 'status': 'pending'}, {'content': 'æŸ¥æ‰¾å…¬å¸çš„ä¸šåŠ¡èŒƒå›´å’Œç»è¥çŠ¶å†µ', 'status': 'pending'}, {'content': 'æ”¶é›†å…¬å¸çš„è”ç³»æ–¹å¼å’Œåœ°å€ä¿¡æ¯', 'status': 'pending'}, {'content': 'æ•´ç†å¹¶æ’°å†™å®Œæ•´çš„å…¬å¸ç ”ç©¶æŠ¥å‘Š', 'status': 'pending'}]}, 'id': 'call_00_GKQG1xixrc5u1dXp2xH2puz0', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 5403, 'output_tokens': 162, 'total_tokens': 5565, 'input_token_details': {'cache_read': 5376}, 'output_token_details': {}})]}}
{'TodoListMiddleware.after_model': None}
{'tools': {'todos': [{'content': 'æœç´¢å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´å…¬å¸çš„åŸºæœ¬ä¿¡æ¯', 'status': 'in_progress'}, {'content': 'æŸ¥è¯¢å…¬å¸çš„å·¥å•†æ³¨å†Œä¿¡æ¯', 'status': 'pending'}, {'content': 'æŸ¥æ‰¾å…¬å¸çš„ä¸šåŠ¡èŒƒå›´å’Œç»è¥çŠ¶å†µ', 'status': 'pending'}, {'content': 'æ”¶é›†å…¬å¸çš„è”ç³»æ–¹å¼å’Œåœ°å€ä¿¡æ¯', 'status': 'pending'}, {'content': 'æ•´ç†å¹¶æ’°å†™å®Œæ•´çš„å…¬å¸ç ”ç©¶æŠ¥å‘Š', 'status': 'pending'}], 'messages': [ToolMessage(content="Updated todo list to [{'content': 'æœç´¢å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´å…¬å¸çš„åŸºæœ¬ä¿¡æ¯', 'status': 'in_progress'}, {'content': 'æŸ¥è¯¢å…¬å¸çš„å·¥å•†æ³¨å†Œä¿¡æ¯', 'status': 'pending'}, {'content': 'æŸ¥æ‰¾å…¬å¸çš„ä¸šåŠ¡èŒƒå›´å’Œç»è¥çŠ¶å†µ', 'status': 'pending'}, {'content': 'æ”¶é›† å…¬å¸çš„è”ç³»æ–¹å¼å’Œåœ°å€ä¿¡æ¯', 'status': 'pending'}, {'content': 'æ•´ç†å¹¶æ’°å†™å®Œæ•´çš„å…¬å¸ç ”ç©¶æŠ¥å‘Š', 'status': 'pending'}]", name='write_todos', id='ae5a906d-5877-4b67-acf2-7ef05ab35fc4', tool_call_id='call_00_GKQG1xixrc5u1dXp2xH2puz0')], 'files': {}}}
{'SummarizationMiddleware.before_model': None}
{'model': {'messages': [AIMessage(content='ç°åœ¨è®©æˆ‘å¼€å§‹æœç´¢è¿™å®¶å…¬å¸çš„åŸºæœ¬ä¿¡æ¯ï¼š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 5667, 'total_tokens': 5726, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 5440}, 'prompt_cache_hit_tokens': 5440, 'prompt_cache_miss_tokens': 227}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': '5f1f31dc-888d-40ea-9c9a-197f85b46953', 'finish_reason': 'tool_calls', 'logprobs': None}, name='deep_agent', id='lc_run--019c120f-2052-7461-8c89-7f1b6217cf50-0', tool_calls=[{'name': 'google_search', 'args': {'query': 'å¹¿è¥¿å—å® å‘¨å®¶çˆ±è²è¯´ å…¬å¸'}, 'id': 'call_00_eZ2ekzI6RgDsEi56cYS9SRos', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 5667, 'output_tokens': 59, 'total_tokens': 5726, 'input_token_details': {'cache_read': 5440}, 'output_token_details': {}})]}}
{'TodoListMiddleware.after_model': None}
{'tools': {'messages': [ToolMessage(content="['ç®€ä»‹ï¼šå¹¿å·ç©—å³°å»ºè®¾å·¥ç¨‹ç›‘ç†æœ‰é™å…¬å¸å¹¿è¥¿åˆ†å…¬å¸ï¼Œæˆç«‹äº2015å¹´
ï¼Œä½äºå¹¿è¥¿å£®æ—è‡ªæ²»åŒºå—å®å¸‚ï¼Œæ˜¯ä¸€å®¶ä»¥ä»äº‹ä¸“ä¸šæŠ€æœ¯æœåŠ¡ä¸šä¸ºä¸»çš„ä¼ä¸šã€‚æ›´å¤š.']", name='google_search', id='e5837400-e7fc-4196-8bd3-d79cfbdbf10f', tool_call_id='call_00_eZ2ekzI6RgDsEi56cYS9SRos')]}}
{'SummarizationMiddleware.before_model': None}
{'model': {'messages': [AIMessage(content='è®©æˆ‘å°è¯•æ›´å…·ä½“çš„æœç´¢ï¼š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 5778, 'total_tokens': 5836, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 5696}, 'prompt_cache_hit_tokens': 5696, 'prompt_cache_miss_tokens': 82}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': 'd67bae7c-78a7-4e28-b717-54ab9a16b5fb', 'finish_reason': 'tool_calls', 'logprobs': None}, name='deep_agent', id='lc_run--019c120f-3674-7e13-a1cc-282ffe3a291e-0', tool_calls=[{'name': 'google_search', 'args': {'query': '"å‘¨å®¶çˆ±è²è¯´" å—å® å…¬å¸'}, 'id': 'call_00_aI3SA0bqwkcxfvCfHbVxwUjb', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 5778, 'output_tokens': 58, 'total_tokens': 5836, 'input_token_details': {'cache_read': 5696}, 'output_token_details': {}})]}}
{'TodoListMiddleware.after_model': None}
{'tools': {'messages': [ToolMessage(content="['... å…¬å¸3å¹¿è¥¿å®¶å®é˜å»ºç­‘è£…é¥°å·¥ç¨‹æœ‰é™å…¬å¸4å—å®äº¬å®æ ¡å¤–æ‰˜ç®¡æœ
åŠ¡æœ‰é™å…¬å¸5å¹¿è¥¿ç›¾é¼æ¶ˆé˜²å™¨ææœ‰é™å…¬å¸6å—å®å¸‚å‘¨å®¶çˆ±è²è¯´æ–‡åŒ–ä¼ æ’­æœ‰é™å…¬å¸7å¹¿è¥¿äº˜ç­‘è£…é¥°æœ‰é™å…¬å¸8å¹¿è¥¿è““ç¡•è´¸æ˜“ ...', '... å…¬å¸6å—å®å¸‚å‘¨å®¶çˆ±è²è¯´æ–‡åŒ–ä¼ æ’­æœ‰é™å…¬å¸7æ­å·è½½å…´ç§‘æŠ€æœ‰é™å…¬å¸8æ™®æ´±åç››è‹±æ‰æ•™è‚²å’¨è¯¢æœ‰é™å…¬å¸9æ¹–å—ä½³æŠ¤åº·å…»è€æœåŠ¡æœ‰é™å…¬å¸10åŒ—æµ·å¯æ¡¢äº§ä¸šæŠ•èµ„åˆä¼™ä¼ä¸šï¼ˆæœ‰é™åˆä¼™ï¼‰ Â· 1æˆ ...', 'å—å®é¤é¥®ä½å®¿å•†æ ‡æ³¨å†Œåç§°å¤§å…¨ ; å‘¨å®¶çˆ± è²è¯´/3ä¸ª, æ³‰å§/3ä¸ª ; å±±ç‰Œæ‘/3ä¸ª, å“èºé¦™/3ä¸ª ; èºçŸ¥é“/3ä¸ª, å°å¹ºé²œ/3ä¸ª ; ç¬‘æºå ‚/3ä¸ª, è™¾æ¯”èŸ¹å¤§/3ä¸ª ; è™¾å®¢æ°”/3ä¸ª, äº”è±¡/3ä¸ª.']", name='google_search', id='89ca0815-cc40-4eb3-ba9e-8c335d358058', tool_call_id='call_00_aI3SA0bqwkcxfvCfHbVxwUjb')]}}
{'SummarizationMiddleware.before_model': None}
{'model': {'messages': [AIMessage(content='å¾ˆå¥½ï¼æˆ‘æ‰¾åˆ°äº†"å—å®å¸‚å‘¨å®¶çˆ±è²è¯´æ–‡åŒ–ä¼ æ’­æœ‰é™å…¬å¸"ã€‚è®©æˆ‘æ›´æ–°ä»»åŠ¡
çŠ¶æ€å¹¶ç»§ç»­æœç´¢æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 6045, 'total_tokens': 6191, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 5824}, 'prompt_cache_hit_tokens': 5824, 'prompt_cache_miss_tokens': 221}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': 'f02fdc45-027a-47ed-be71-c128f61ab475', 'finish_reason': 'tool_calls', 'logprobs': None}, name='deep_agent', id='lc_run--019c120f-4dc6-7583-856e-9fb2282d9861-0', tool_calls=[{'name': 'write_todos', 'args': {'todos': [{'content': 'æœç´¢å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´å…¬å¸çš„åŸºæœ¬ä¿¡æ¯', 'status': 'completed'}, {'content': 'æŸ¥è¯¢å…¬å¸çš„å·¥å•†æ³¨å†Œä¿¡æ¯', 'status': 'in_progress'}, {'content': 'æŸ¥æ‰¾ å…¬å¸çš„ä¸šåŠ¡èŒƒå›´å’Œç»è¥çŠ¶å†µ', 'status': 'pending'}, {'content': 'æ”¶é›†å…¬å¸çš„è”ç³»æ–¹å¼å’Œåœ°å€ä¿¡æ¯', 'status': 'pending'}, {'content': 'æ•´ç†å¹¶æ’°å†™å®Œæ•´çš„å…¬å¸ç ”ç©¶æŠ¥å‘Š', 'status': 'pending'}]}, 'id': 'call_00_coybBhwuiTry7H1qRW8cONUb', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 6045, 'output_tokens': 146, 'total_tokens': 6191, 'input_token_details': {'cache_read': 5824}, 'output_token_details': {}})]}}
{'TodoListMiddleware.after_model': None}
{'tools': {'todos': [{'content': 'æœç´¢å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´å…¬å¸çš„åŸºæœ¬ä¿¡æ¯', 'status': 'completed'}, {'content': 'æŸ¥è¯¢å…¬å¸çš„å·¥å•†æ³¨å†Œä¿¡æ¯', 'status': 'in_progress'}, {'content': 'æŸ¥æ‰¾å…¬å¸çš„ä¸šåŠ¡èŒƒå›´å’Œç»è¥çŠ¶å†µ', 'status': 'pending'}, {'content': 'æ”¶é›†å…¬å¸çš„è”ç³»æ–¹å¼å’Œåœ°å€ä¿¡æ¯', 'status': 'pending'}, {'content': 'æ•´ç†å¹¶æ’°å†™å®Œæ•´çš„å…¬å¸ç ”ç©¶æŠ¥å‘Š', 'status': 'pending'}], 'messages': [ToolMessage(content="Updated todo list to [{'content': 'æœç´¢å¹¿è¥¿å—å®å‘¨å®¶çˆ±è²è¯´å…¬å¸çš„åŸºæœ¬ä¿¡æ¯', 'status': 'completed'}, {'content': 'æŸ¥è¯¢å…¬å¸çš„å·¥å•†æ³¨å†Œä¿¡æ¯', 'status': 'in_progress'}, {'content': 'æŸ¥æ‰¾å…¬å¸çš„ä¸šåŠ¡èŒƒå›´å’Œç»è¥çŠ¶å†µ', 'status': 'pending'}, {'content': ' æ”¶é›†å…¬å¸çš„è”ç³»æ–¹å¼å’Œåœ°å€ä¿¡æ¯', 'status': 'pending'}, {'content': 'æ•´ç†å¹¶æ’°å†™å®Œæ•´çš„å…¬å¸ç ”ç©¶æŠ¥å‘Š', 'status': 'pending'}]", name='write_todos', id='5025ada3-6d69-4373-9f68-4722eb95a16d', tool_call_id='call_00_coybBhwuiTry7H1qRW8cONUb')], 'files': {}}}
{'SummarizationMiddleware.before_model': None}
{'model': {'messages': [AIMessage(content='ç°åœ¨è®©æˆ‘æœç´¢è¿™å®¶å…¬å¸çš„å·¥å•†æ³¨å†Œä¿¡æ¯ï¼š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 6308, 'total_tokens': 6373, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 6144}, 'prompt_cache_hit_tokens': 6144, 'prompt_cache_miss_tokens': 164}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': 'e5b23b84-0a50-41f7-b33e-063a7a1544bb', 'finish_reason': 'tool_calls', 'logprobs': None}, name='deep_agent', id='lc_run--019c120f-6288-7630-8294-737fee17b16b-0', tool_calls=[{'name': 'google_search', 'args': {'query': '"å—å®å¸‚å‘¨å®¶çˆ±è²è¯´æ–‡åŒ–ä¼ æ’­æœ‰é™å…¬å¸" å·¥å•†æ³¨å†Œ'}, 'id': 'call_00_xNSp06AEosq8CseFUjazt1Qk', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 6308, 'output_tokens': 65, 'total_tokens': 6373, 'input_token_details': {'cache_read': 6144}, 'output_token_details': {}})]}}
{'TodoListMiddleware.after_model': None}
# å­ agent

# ä¸–ç•Œä¸Šç›¸å½“äºè°ƒç”¨task å·¥å…·åˆ›å»ºå­ä»£ç†å¹¶æ‰§è¡Œä»»åŠ¡
tool_calls=[{'name': 'task', 'args': {'subagent_type': 'general-purpose', 
'description': 'è¯·ç ”ç©¶è‰è“çš„ä¸»è¦æˆç†Ÿå­£èŠ‚ã€‚éœ€è¦åŒ…æ‹¬ä»¥ä¸‹ä¿¡æ¯ï¼š\n1. è‰è“åœ¨ä¸­å›½çš„ä¸»è¦ç§æ¤åŒºåŸŸ\n2. è‰è“çš„ä¸»è¦æˆç†Ÿå­£èŠ‚ï¼ˆåŒ…æ‹¬ä¸åŒåœ°åŒºçš„å·®å¼‚ï¼‰\n3.  å½±å“è‰è“æˆç†Ÿæ—¶é—´çš„å› ç´ 
\n4. è‰è“çš„é‡‡æ‘˜æœŸé€šå¸¸æŒç»­å¤šä¹…\n5. æ˜¯å¦æœ‰åå­£èŠ‚è‰è“ç§æ¤ï¼Œå¦‚æœæœ‰ï¼Œæ˜¯å¦‚ä½•å®ç°çš„\n\nè¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ä¿¡æ¯ï¼Œå¹¶æ³¨æ˜ä¿¡æ¯æ¥æºæˆ–åŸºäºå¸¸è¯†çš„åˆç†æ¨æ–­ã€‚'}, 
'id': 'call_00_vxLE7FzNsZE5poK20fm4VhD3', 'type': 'tool_call'}

#å…¨å“åº”
{'PatchToolCallsMiddleware.before_agent': {'messages': Overwrite(value=[HumanMessage(content='è¯·åŒæ—¶å¸®å¿™åˆ†æè‰è“å’Œæ¨æ¢…çš„ä¸»è¦æˆç†Ÿå­£èŠ‚', additional_kwargs={}, response_metadata={}, id='1991837a-41f3-402b-b0a5-836d5a944f1f')])}}
{'SummarizationMiddleware.before_model': None}
{'model': {'messages': [AIMessage(content='æˆ‘å°†åŒæ—¶åˆ†æè‰è“å’Œæ¨æ¢…çš„ä¸»è¦æˆç†Ÿå­£èŠ‚ã€‚è®©æˆ‘å¯åŠ¨ä¸¤ä¸ªå¹¶è¡Œä»»åŠ¡æ¥åˆ†åˆ«ç ”ç©¶è¿™ä¸¤ç§æ°´æœçš„æˆç†Ÿå­£èŠ‚ã€‚', 
additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 303, 'prompt_tokens': 5145, 'total_tokens': 5448, 'completion_tokens_details': None, 
'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 320}, 'prompt_cache_hit_tokens': 320, 'prompt_cache_miss_tokens': 4825}, 'model_provider': 'openai', 'model_name': 
'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': 'c21b4b8f-c669-4f3e-ab0f-369fc2a864dc', 'finish_reason': 'tool_calls', 'logprobs': None}, 
name='deep_agent', id='lc_run--019c121a-3e1e-7dd2-a4b1-75631dff10ec-0', tool_calls=[{'name': 'task', 'args': {'subagent_type': 'general-purpose', 
'description': 'è¯·ç ”ç©¶è‰è“çš„ä¸»è¦æˆç†Ÿå­£èŠ‚ã€‚éœ€è¦åŒ…æ‹¬ä»¥ä¸‹ä¿¡æ¯ï¼š\n1. è‰è“åœ¨ä¸­å›½çš„ä¸»è¦ç§æ¤åŒºåŸŸ\n2. è‰è“çš„ä¸»è¦æˆç†Ÿå­£èŠ‚ï¼ˆåŒ…æ‹¬ä¸åŒåœ°åŒºçš„å·®å¼‚ï¼‰\n3.  å½±å“è‰è“æˆç†Ÿæ—¶é—´çš„å› ç´ 
\n4. è‰è“çš„é‡‡æ‘˜æœŸé€šå¸¸æŒç»­å¤šä¹…\n5. æ˜¯å¦æœ‰åå­£èŠ‚è‰è“ç§æ¤ï¼Œå¦‚æœæœ‰ï¼Œæ˜¯å¦‚ä½•å®ç°çš„\n\nè¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ä¿¡æ¯ï¼Œå¹¶æ³¨æ˜ä¿¡æ¯æ¥æºæˆ–åŸºäºå¸¸è¯†çš„åˆç†æ¨æ–­ã€‚'}, 
'id': 'call_00_vxLE7FzNsZE5poK20fm4VhD3', 'type': 'tool_call'}, {'name': 'task', 'args': {'subagent_type': 'general-purpose', 'description': 'è¯·ç ”ç©¶æ¨æ¢…çš„ä¸»è¦æˆç†Ÿå­£èŠ‚ã€‚éœ€è¦åŒ…æ‹¬ä»¥ä¸‹ä¿¡æ¯ï¼š
\n1. æ¨æ¢…åœ¨ä¸­å›½çš„ä¸»è¦ç§æ¤åŒºåŸŸ\n2. æ¨æ¢…çš„ä¸»è¦æˆç†Ÿå­£èŠ‚ï¼ˆåŒ…æ‹¬ä¸åŒåœ°åŒºçš„å·®å¼‚ï¼‰\n3. å½±å“æ¨æ¢…æˆç†Ÿæ—¶é—´çš„å› ç´ \n4. æ¨æ¢…çš„é‡‡æ‘˜æœŸé€šå¸¸æŒç»­å¤šä¹…\n5. æ¨æ¢…çš„å“ç§å·®å¼‚å¯¹æˆç†Ÿæ—¶é—´çš„å½±å“\n\nè¯·æä¾›è¯¦ ç»†ã€å‡†ç¡®çš„ä¿¡æ¯ï¼Œ
å¹¶æ³¨æ˜ä¿¡æ¯æ¥æºæˆ–åŸºäºå¸¸è¯†çš„åˆç†æ¨æ–­ã€‚'}, 'id': 'call_01_jxyYKKbInGcDFeDF6Qzgy8su', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 5145, 'output_tokens': 
303, 'total_tokens': 5448, 'input_token_details': {'cache_read': 320}, 'output_token_details': {}})]}}
{'TodoListMiddleware.after_model': None}

6.RAG
6.RAG
from langchain.agents.middleware import ToolRetryMiddleware, LLMToolEmulator
if __name__ == "__main__":
    pass
